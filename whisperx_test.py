# transcribe_diarize.py
import os, argparse, pathlib
from pathlib import Path
import torch, whisperx
from whisperx.diarize import DiarizationPipeline
import huggingface_hub
huggingface_hub.login("hf_")

# Windows ì „ìš©: PyTorch DLL ê²½ë¡œ ìš°ì„ 
try:
    os.add_dll_directory(str(pathlib.Path(torch.__file__).parents[1] / "lib"))
except Exception:
    pass

# ì˜µì…˜: ì†ë„ í–¥ìƒ(TF32). í•„ìš” ì—†ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬.
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

def mmss(t):
    t = 0.0 if not isinstance(t, (int, float)) or t != t or t < 0 else float(t)
    m = int(t // 60)
    s = int(round(t - m * 60))
    if s == 60:
        m += 1
        s = 0
    return f"{m:02d}:{s:02d}"

def write_lines(p: Path, lines):
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        for ln in lines:
            f.write(ln + "\n")

def build_diar_pipeline(token: str, prefer_device: str):
    try:
        return DiarizationPipeline(use_auth_token=token, device=prefer_device)
    except OSError as e:
        print(f"[WARN] GPU diarization ì‹¤íŒ¨: {e}\n[INFO] CPUë¡œ í´ë°±.")
        return DiarizationPipeline(use_auth_token=token, device="cpu")

def looks_blackwell(gpu_name: str):
    n = gpu_name.lower()
    keys = ["rtx 50", "rtx5", "5070", "5080", "5090", "blackwell", "gb2", "gb20"]
    return any(k in n for k in keys)

def process_one(fp: Path, out_dir: Path, model, diar_pipe, device: str, batch: int,
                min_spk, max_spk, load_model_fn, compute_type: str):
    audio = whisperx.load_audio(str(fp))

    # 1) ASR(+ì–¸ì–´ê°ì§€). INT8 ê²½ë¡œ(cuBLAS) ì‹¤íŒ¨ ì‹œ FP16ìœ¼ë¡œ 1íšŒ ìë™ ì¬ì‹œë„.
    try:
        asr = model.transcribe(audio, batch_size=batch)
    except RuntimeError as e:
        msg = str(e).lower()
        if ("cublas_status_not_supported" in msg or "cublas" in msg) and "int8" in (compute_type or ""):
            print("[INFO] cuBLAS INT8 ë¬¸ì œ ê°ì§€. FP16ìœ¼ë¡œ ì¬ì‹œë„.")
            model = load_model_fn("float16")
            asr = model.transcribe(audio, batch_size=batch)
        else:
            raise

    lang = asr.get("language", "ko")

    # 2) ì •ë ¬
    align_model, meta = whisperx.load_align_model(language_code=lang, device=device)
    asr_aligned = whisperx.align(
        asr["segments"], align_model, meta, audio, device, return_char_alignments=False
    )

    # 3) í™”ìë¶„ë¦¬
    diar_kwargs = {}
    if min_spk is not None:
        diar_kwargs["min_speakers"] = min_spk
    if max_spk is not None:
        diar_kwargs["max_speakers"] = max_spk
    diar_segments = diar_pipe(audio, **diar_kwargs)
    asr_spk = whisperx.assign_word_speakers(diar_segments, asr_aligned)

    # 4) ì €ì¥: [mm:ss] í™”ìN : í…ìŠ¤íŠ¸
    spk_map, lines = {}, []
    for seg in sorted(asr_spk["segments"], key=lambda x: x.get("start", 0.0)):
        txt = (seg.get("text") or "").strip()
        if not txt:
            continue
        spk = seg.get("speaker", "UNK")
        if spk not in spk_map:
            spk_map[spk] = f"í™”ì{len(spk_map) + 1}"
        lines.append(f"[{mmss(seg.get('start', 0.0))}] {spk_map[spk]} : {txt}")

    out_path = out_dir / f"{fp.stem}_transcript_{lang}.txt"
    write_lines(out_path, lines)
    return out_path

def main():
    ap = argparse.ArgumentParser(
        description="WhisperX + diarization ë°°ì¹˜ ì²˜ë¦¬ (ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í˜•ì‹ ì§€ì›)",
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python whisperx_test.py --src audio_folder                    # í´ë”ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬
  python whisperx_test.py --src my_audio.wav                    # ë‹¨ì¼ WAV íŒŒì¼ ì²˜ë¦¬
  python whisperx_test.py --src audio_folder --ext "*.mp3"      # MP3 íŒŒì¼ë§Œ ì²˜ë¦¬
  python whisperx_test.py --src audio_folder --ext "*.wav"      # WAV íŒŒì¼ë§Œ ì²˜ë¦¬
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    ap.add_argument("--src", default="video_input", help="ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ íŒŒì¼ ë˜ëŠ” í´ë” (mp4, wav, mp3, flac, m4a, aac, ogg ì§€ì›)")
    ap.add_argument("--out_dir", default="input", help="ì¶œë ¥ í´ë”")
    ap.add_argument("--whisper_model", default="large-v3", help="Whisper ëª¨ë¸ëª…")
    ap.add_argument("--batch_size", type=int, default=8)
    ap.add_argument("--min_speakers", type=int, default=None)
    ap.add_argument("--max_speakers", type=int, default=None)
    ap.add_argument("--hf_token", default=None, help="Hugging Face read í† í°")
    ap.add_argument("--force_cpu_diar", action="store_true", help="í™”ìë¶„ë¦¬ë§Œ CPU ê°•ì œ")
    ap.add_argument("--compute_type", default=None, help='CUDA: float16|bfloat16|float32|int8_float16 ë“±')
    ap.add_argument("--ext", default="*.mp4", help="ê²€ìƒ‰ í™•ì¥ì íŒ¨í„´ (ì˜ˆ: *.mp4, *.wav, *.mp3)")
    args = ap.parse_args()

    token = (
        args.hf_token
        or os.getenv("HUGGINGFACE_HUB_TOKEN")
        or os.getenv("HUGGINGFACE_TOKEN")
        or os.getenv("HF_TOKEN")
    )

    # GPU cuDNN ë¬¸ì œ ìš°íšŒë¥¼ ìœ„í•´ CPU ì‚¬ìš©
    device = "cpu"  # "cuda" if torch.cuda.is_available() else "cpu"
    gpu_name = torch.cuda.get_device_name(0) if device == "cuda" else ""
    is_blackwell = device == "cuda" and looks_blackwell(gpu_name)

    # ê¸°ë³¸ ì •ë°€ë„ ì„ íƒ: Blackwell(RTX 50xx)ì—ì„œëŠ” INT8 ê³„ì—´ íšŒí”¼
    if args.compute_type:
        compute_type = args.compute_type
    else:
        if device == "cuda":
            compute_type = "float16" if is_blackwell else "int8_float16"
        else:
            compute_type = "int8"

    def load_model_with(ct):
        return whisperx.load_model(args.whisper_model, device=device, compute_type=ct)

    # ëª¨ë¸ ë¡œë“œ
    model = load_model_with(compute_type)

    diar_device = "cpu" if args.force_cpu_diar else device
    diar_pipe = build_diar_pipeline(token, diar_device)

    # ì…ë ¥ ìˆ˜ì§‘
    src_path = Path(args.src)
    
    if src_path.is_file():
        files = [src_path]
    else:
        # ì—¬ëŸ¬ ì˜¤ë””ì˜¤ í˜•ì‹ ì§€ì›
        audio_extensions = ["*.mp4", "*.wav", "*.mp3", "*.flac", "*.m4a", "*.aac", "*.ogg"]
        files = []
        
        if args.ext != "*.mp4":  # ì‚¬ìš©ìê°€ íŠ¹ì • í™•ì¥ìë¥¼ ì§€ì •í•œ ê²½ìš°
            files = sorted(src_path.glob(args.ext))
        else:  # ê¸°ë³¸ê°’ì¸ ê²½ìš° ëª¨ë“  ì˜¤ë””ì˜¤ í˜•ì‹ ê²€ìƒ‰
            for ext in audio_extensions:
                files.extend(src_path.glob(ext))
            files = sorted(set(files))  # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    
    assert files, f"{src_path}ì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì§€ ëª»í•¨. ì§€ì› í˜•ì‹: {', '.join(audio_extensions)}"
    
    # ì°¾ì€ íŒŒì¼ë“¤ í‘œì‹œ
    print(f"\nğŸ“ ì²˜ë¦¬í•  íŒŒì¼ {len(files)}ê°œ:")
    for i, fp in enumerate(files, 1):
        print(f"  {i}. {fp.name} ({fp.suffix.upper()})")
    print()

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    for fp in files:
        try:
            outp = process_one(
                fp, out_dir, model, diar_pipe, device, args.batch_size,
                args.min_speakers, args.max_speakers, load_model_with, compute_type
            )
            print(f"OK: {fp.name} -> {outp}")
        except Exception as e:
            print(f"ERR: {fp.name}: {e}")

if __name__ == "__main__":
    main()
