#!/usr/bin/env python3
"""
ìŒì„± íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±° â†’ Whisper STT â†’ ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ â†’ í…ìŠ¤íŠ¸ ì €ì¥ íŒŒì´í”„ë¼ì¸

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:
- librosa: ê³ ê¸‰ ìŒì„± íŠ¹ì„± ì¶”ì¶œ (pip install librosa)
- scikit-learn: í´ëŸ¬ìŠ¤í„°ë§ (pip install scikit-learn)
- numpy: ê¸°ë³¸ ìˆ˜ì¹˜ ì—°ì‚° (pip install numpy)
- soundfile: ì„ì‹œ WAV íŒŒì¼ ì €ì¥ (pip install soundfile)
- tempfile: ì„ì‹œ íŒŒì¼ ìƒì„± (pip install tempfile)
"""

import os
import sys
import glob
import logging
import shutil
import tempfile
from pathlib import Path
from datetime import datetime
import torch
import torchaudio
import whisper
import soundfile as sf
import numpy as np
from typing import Optional, Dict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('audio_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# SpeechBrain importë¥¼ ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
try:
    from speechbrain.inference.enhancement import SpectralMaskEnhancement
    from speechbrain.inference.speaker import EncoderClassifier
    from speechbrain.utils.fetching import LocalStrategy
    import numpy as np
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.metrics.pairwise import cosine_similarity
    SPEECHBRAIN_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SpeechBrain ë¡œë”© ì‹¤íŒ¨: {e}")
    SPEECHBRAIN_AVAILABLE = False

# WhisperX + pyannote diarization ì‚¬ìš© ì—¬ë¶€
try:
    import whisperx
    WHISPERX_AVAILABLE = True
except ImportError as e:
    logger.warning(f"WhisperX ë¡œë”© ì‹¤íŒ¨ (pyannote í™”ìë¶„ë¦¬ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤): {e}")
    WHISPERX_AVAILABLE = False

class AudioPipeline:
    """ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, use_gpu=True, target_language=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            use_gpu (bool): GPU ì‚¬ìš© ì—¬ë¶€
            target_language (str): ëŒ€ìƒ ì–¸ì–´ ('ko', 'ja', 'en', None=ìë™ê°ì§€)
        """
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.target_language = target_language
        
        # WhisperX + pyannote ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ì½ê¸°)
        self.pyannote_auth_token = os.getenv("PYANNOTE_AUTH_TOKEN", None)
        self.use_whisperx_diarization = True
        
        # ìµœëŒ€ í™”ì ìˆ˜ ì„¤ì • (1~10 ë²”ìœ„ì—ì„œ ì‚¬ìš©)
        self.max_speakers = 5

        # VAD(Voice Activity Detection) ì‚¬ìš© ì—¬ë¶€
        # Trueì´ë©´ ë…¸ì´ì¦ˆ ì œê±° ì´í›„ì— ì•/ë’¤ ë¬´ìŒì„ ì˜ë¼ì„œ STT íš¨ìœ¨ì„ ë†’ì…ë‹ˆë‹¤.
        self.enable_vad = True

        # í´ë” ê²½ë¡œ ì„¤ì •
        self.audio_input_dir = Path("audio_input")
        self.audio_out_dir = Path("audio_out") 
        self.audio_output_dir = Path("audio_output")
        self.script_output_dir = Path("script_output")
        
        # í´ë” ìƒì„±
        self._create_directories()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.denoiser = None
        self.whisper_model = None
        self.speaker_encoder = None  # ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸
        
        # ì§€ì› ì–¸ì–´ ì •ë³´ (í•œêµ­ì–´, ì¼ë³¸ì–´, ì˜ì–´ë§Œ ì§€ì›)
        self.supported_languages = {
            'ko': 'í•œêµ­ì–´',
            'ja': 'æ—¥æœ¬èª',
            'en': 'English'
        }
        
        # ì§€ì› ì˜¤ë””ì˜¤ íŒŒì¼ í˜•ì‹ (ì˜ìƒì€ Electron ìª½ì—ì„œ WAVë¡œ ë³€í™˜ë¨)
        self.audio_formats = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac']
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - ë””ë°”ì´ìŠ¤: {self.device}")
        if target_language:
            lang_name = self.supported_languages.get(target_language, target_language)
            logger.info(f"ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
        else:
            logger.info("ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for directory in [self.audio_input_dir, self.audio_out_dir, 
                         self.audio_output_dir, self.script_output_dir]:
            directory.mkdir(exist_ok=True)
            logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±/í™•ì¸: {directory}")
    
    def _is_audio_file(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return Path(file_path).suffix.lower() in self.audio_formats
    
    def _load_denoiser(self):
        """ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë“œ"""
        if self.denoiser is None:
            if not SPEECHBRAIN_AVAILABLE:
                logger.warning("SpeechBrainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©")
                self._load_denoiser_alternative()
                return
                
            try:
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # Windows ê¶Œí•œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ LocalStrategy ì‚¬ìš©
                import os
                os.environ['SPEECHBRAIN_CACHE_STRATEGY'] = 'LOCAL'
                
                # ì ˆëŒ€ ê²½ë¡œë¡œ savedir ì„¤ì •
                savedir = os.path.abspath("pretrained_models/metricgan-plus-voicebank")
                
                self.denoiser = SpectralMaskEnhancement.from_hparams(
                    source="speechbrain/metricgan-plus-voicebank",
                    savedir=savedir,
                    run_opts={"device": self.device}
                )
                logger.info("ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ì¬ì‹œë„...")
                try:
                    self._load_denoiser_alternative()
                except Exception as e2:
                    logger.error(f"ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
                    raise e
    
    def _load_denoiser_alternative(self):
        """ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• (ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        logger.info("ëŒ€ì•ˆ ë…¸ì´ì¦ˆ ì œê±° ë°©ë²• ì‚¬ìš© (ê°„ë‹¨í•œ í•„í„°ë§)")
        self.denoiser = "simple_filter"  # í”Œë˜ê·¸ë¡œ ì‚¬ìš©
    
    def _simple_denoise(self, waveform, sample_rate):
        """ê°„ë‹¨í•œ ë…¸ì´ì¦ˆ ì œê±° (ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§)"""
        try:
            # ê°„ë‹¨í•œ ê³ ì—­ í†µê³¼ í•„í„° ì ìš©
            from scipy.signal import butter, filtfilt
            
            # 80Hz ì´í•˜ ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±°
            nyquist = sample_rate / 2
            low_cutoff = 80 / nyquist
            b, a = butter(4, low_cutoff, btype='high')
            
            # í•„í„° ì ìš©
            filtered = filtfilt(b, a, waveform.numpy())
            
            # ì •ê·œí™”
            filtered = filtered / np.max(np.abs(filtered)) * 0.8
            
            return torch.from_numpy(filtered).float()
            
        except ImportError:
            logger.warning("scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©")
            # ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©
            normalized = waveform / torch.max(torch.abs(waveform)) * 0.8
            return normalized

    def _apply_vad(self, waveform, sample_rate, energy_threshold: float = 0.02):
        """
        ë§¤ìš° ë‹¨ìˆœí•œ ì—ë„ˆì§€ ê¸°ë°˜ VAD.
        - ì…ë ¥: ë‹¨ì¼ ì±„ë„ waveform (1, N)
        - ì¶œë ¥: ì•/ë’¤ ë¬´ìŒì´ ì˜ë ¤ì§„ waveform (ë‚´ë¶€ ê¸´ ë¬´ìŒì€ ìœ ì§€)

        Args:
            waveform: 1ì±„ë„ ì˜¤ë””ì˜¤ í…ì„œ (1, N)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (Hz)
            energy_threshold: ìµœëŒ€ ì—ë„ˆì§€ ëŒ€ë¹„ ë§ì†Œë¦¬ë¡œ ë³¼ ìµœì†Œ ë¹„ìœ¨ (0.0 ~ 1.0)
        """
        try:
            # ëª¨ë…¸ ë³´ì¥
            if waveform.dim() == 2 and waveform.size(0) > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)

            # ì•„ì£¼ ì§§ì€ ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if waveform.size(-1) < sample_rate * 0.3:
                return waveform

            window_size = int(sample_rate * 0.03)  # 30 ms
            hop_size = max(1, window_size // 2)
            num_samples = waveform.size(-1)

            energies = []
            for start in range(0, num_samples, hop_size):
                end = min(start + window_size, num_samples)
                frame = waveform[..., start:end]
                if frame.numel() == 0:
                    break
                # RMS ì—ë„ˆì§€
                energy = torch.sqrt(torch.mean(frame ** 2))
                energies.append(energy.item())

            if not energies:
                return waveform

            energies_tensor = torch.tensor(energies)
            max_energy = float(energies_tensor.max())
            if max_energy <= 0:
                return waveform

            threshold = max_energy * float(energy_threshold)
            speech_indices = (energies_tensor > threshold).nonzero(as_tuple=False).flatten()
            if speech_indices.numel() == 0:
                # ì „ë¶€ ë¬´ìŒìœ¼ë¡œ íŒë‹¨ë˜ë©´ ì›ë³¸ ìœ ì§€
                return waveform

            first_idx = int(speech_indices[0])
            last_idx = int(speech_indices[-1])
            start_sample = max(0, first_idx * hop_size)
            end_sample = min(num_samples, (last_idx + 1) * hop_size)

            trimmed = waveform[..., start_sample:end_sample]
            logger.info(
                f"VAD íŠ¸ë¦¬ë°: {num_samples / sample_rate:.2f}s â†’ "
                f"{trimmed.size(-1) / sample_rate:.2f}s"
            )
            return trimmed
        except Exception as e:
            logger.warning(f"VAD ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ì›ë³¸ waveform ì‚¬ìš©: {e}")
            return waveform

    def _load_whisper(self, model_size="large-v3"):
        """Whisper ëª¨ë¸ ë¡œë“œ"""
        if self.whisper_model is None:
            try:
                logger.info(f"Whisper ëª¨ë¸ ({model_size}) ë¡œë”© ì¤‘...")
                self.whisper_model = whisper.load_model(model_size, device=self.device)
                logger.info("Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
    
    def _load_speaker_encoder(self):
        """ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ"""
        if self.speaker_encoder is None:
            if not SPEECHBRAIN_AVAILABLE:
                logger.warning("SpeechBrainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í™”ìë¶„ë¦¬ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
                
            try:
                logger.info("ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì¤‘...")
                
                # ê¶Œí•œ ë¬¸ì œ ìš°íšŒë¥¼ ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                import tempfile
                temp_dir = tempfile.mkdtemp(prefix="speechbrain_")
                
                # ECAPA-VOXCELEB ëª¨ë¸ ë¡œë“œ
                self.speaker_encoder = EncoderClassifier.from_hparams(
                    source="speechbrain/spkrec-ecapa-voxceleb",
                    savedir=temp_dir,
                    run_opts={"device": self.device}
                )
                logger.info("ECAPA-VOXCELEB í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                return True
            except Exception as e:
                logger.error(f"í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                logger.info("ê¶Œí•œ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.speaker_encoder = None
                return False
        return True
    
    def denoise_audio(self, input_file, output_file):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë…¸ì´ì¦ˆ ì œê±°
        
        Args:
            input_file (str): ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘: {input_file}")
            
            # ëª¨ë¸ ë¡œë“œ
            self._load_denoiser()
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(input_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ 16kHzë¡œ ë³€í™˜ (SpeechBrain ëª¨ë¸ ìš”êµ¬ì‚¬í•­)
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            
            # ë…¸ì´ì¦ˆ ì œê±° ìˆ˜í–‰
            if self.denoiser == "simple_filter":
                # ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©
                logger.info("ê°„ë‹¨í•œ í•„í„°ë§ ë°©ë²•ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°")
                enhanced_waveform = self._simple_denoise(waveform.squeeze(0), sample_rate)
                enhanced_waveform = enhanced_waveform.unsqueeze(0)
            else:
                # SpeechBrain ëª¨ë¸ ì‚¬ìš©
                waveform = waveform.to(self.device)
                enhanced_waveform = self.denoiser.enhance_batch(waveform.unsqueeze(0))
                enhanced_waveform = enhanced_waveform.squeeze(0).cpu()

            # VAD ê¸°ë°˜ ì•/ë’¤ ë¬´ìŒ ì œê±° (ì˜µì…˜)
            if getattr(self, "enable_vad", False):
                try:
                    logger.info("VAD ê¸°ë°˜ ë¬´ìŒ êµ¬ê°„ íŠ¸ë¦¬ë° ìˆ˜í–‰")
                    enhanced_waveform = self._apply_vad(enhanced_waveform, sample_rate)
                except Exception as e:
                    logger.warning(f"VAD ì ìš© ì‹¤íŒ¨, ì›ë³¸ waveform ì‚¬ìš©: {e}")

            # ì¶œë ¥ íŒŒì¼ ì €ì¥
            torchaudio.save(output_file, enhanced_waveform, sample_rate)
            
            logger.info(f"ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ: {output_file}")
            
        except Exception as e:
            logger.error(f"ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨ ({input_file}): {e}")
            raise
    
    def transcribe_audio(self, audio_file, output_text_file, srt_file=None, diarization_audio_file=None, enable_timestamps=True):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (STT)
        
        Args:
            audio_file (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_text_file (str): ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
            srt_file (str, optional): SRT ìë§‰ íŒŒì¼ ê²½ë¡œ
            diarization_audio_file (str, optional): í™”ìë¶„ë¦¬ìš© ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        try:
            logger.info(f"STT ì²˜ë¦¬ ì‹œì‘: {audio_file}")
            
            # Whisper ëª¨ë¸ ë¡œë“œ
            self._load_whisper()
            
            # ìŒì„± ì¸ì‹ ì˜µì…˜ (ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            # condition_on_previous_text=False ë¡œ ì„¤ì •í•˜ì—¬
            # ê¸´ ì¹¨ë¬µ ì´í›„ ë™ì¼ ë¬¸ì¥ ë°˜ë³µ/í™˜ê°ì„ ì¤„ì¸ë‹¤.
            transcribe_options = {
                "word_timestamps": False,
                "verbose": True,
                "condition_on_previous_text": False,
            }
            
            # ì–¸ì–´ ì„¤ì • (ìœ íš¨í•œ ì–¸ì–´ ì½”ë“œë§Œ ì‚¬ìš©)
            if self.target_language and self.target_language in self.supported_languages:
                transcribe_options["language"] = self.target_language
                lang_name = self.supported_languages.get(self.target_language, self.target_language)
                logger.info(f"ì§€ì •ëœ ì–¸ì–´ë¡œ STT ì²˜ë¦¬: {lang_name}")
            else:
                if self.target_language:
                    logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì–¸ì–´ ì½”ë“œ ë¬´ì‹œ: {self.target_language} (ìë™ ê°ì§€ ëª¨ë“œë¡œ ì „í™˜)")
                logger.info("ì–¸ì–´ ìë™ ê°ì§€ë¡œ STT ì²˜ë¦¬")
            
            # ê¸´ ì˜¤ë””ì˜¤ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            total_duration = None
            sample_rate = None
            num_frames = None
            try:
                info = torchaudio.info(audio_file)
                sample_rate = info.sample_rate
                num_frames = info.num_frames
                if sample_rate and num_frames:
                    total_duration = num_frames / float(sample_rate)
            except Exception as e:
                logger.warning(f"ì˜¤ë””ì˜¤ ë©”íƒ€ë°ì´í„° í™•ì¸ ì‹¤íŒ¨, ë‹¨ì¼ íŒŒì¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {e}")
            
            # chunk ê¸°ì¤€ (ì´ˆ ë‹¨ìœ„, 10ë¶„)
            chunk_duration = 10800.0
            
            if total_duration is None or total_duration <= chunk_duration:
                # ê¸°ì¡´ ë°©ì‹: ì „ì²´ íŒŒì¼ì„ í•œ ë²ˆì— ì²˜ë¦¬
                result = self.whisper_model.transcribe(str(audio_file), **transcribe_options)
            else:
                logger.info(
                    f"ê¸´ ì˜¤ë””ì˜¤ ê°ì§€ ({total_duration/60.0:.1f}ë¶„) - "
                    f"{chunk_duration/60.0:.0f}ë¶„ ë‹¨ìœ„ chunkë¡œ ë¶„í•  ì²˜ë¦¬"
                )

                merged_segments = []
                text_parts = []
                chunk_results = []

                # ìƒ˜í”Œ ê¸°ì¤€ chunk í¬ê¸°
                chunk_samples = int(chunk_duration * sample_rate)
                offset_sec = 0.0

                start_frame = 0
                while start_frame < num_frames:
                    remaining = num_frames - start_frame
                    this_frames = min(chunk_samples, remaining)

                    logger.info(
                        f"chunk STT ì²˜ë¦¬: start_frame={start_frame}, "
                        f"frames={this_frames}, offset={offset_sec:.2f}s"
                    )

                    # í•´ë‹¹ chunkë§Œ ë¡œë“œ
                    waveform, sr = torchaudio.load(audio_file, frame_offset=start_frame, num_frames=this_frames)

                    # ëª¨ë…¸ ë³€í™˜
                    if waveform.shape[0] > 1:
                        waveform = torch.mean(waveform, dim=0, keepdim=True)

                    # numpyë¡œ ë³€í™˜í•˜ì—¬ ì„ì‹œ wav íŒŒì¼ë¡œ ì €ì¥ í›„ Whisper í˜¸ì¶œ
                    audio_np = waveform.squeeze(0).numpy()

                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
                        tmp_path = tmp_wav.name
                    try:
                        sf.write(tmp_path, audio_np, sr)
                        chunk_result = self.whisper_model.transcribe(tmp_path, **transcribe_options)
                    finally:
                        try:
                            os.remove(tmp_path)
                        except Exception:
                            pass

                    chunk_results.append(chunk_result)

                    # í…ìŠ¤íŠ¸ ëˆ„ì 
                    chunk_text = chunk_result.get("text", "").strip()
                    if chunk_text:
                        text_parts.append(chunk_text)

                    # ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ì— offset ì ìš© í›„ ë³‘í•©
                    if "segments" in chunk_result:
                        for seg in chunk_result["segments"]:
                            seg_copy = dict(seg)
                            seg_copy["start"] = float(seg_copy.get("start", 0.0)) + offset_sec
                            seg_copy["end"] = float(seg_copy.get("end", seg_copy.get("start", 0.0))) + offset_sec

                            if "words" in seg_copy:
                                words = []
                                for w in seg_copy["words"]:
                                    w_copy = dict(w)
                                    w_copy["start"] = float(w_copy.get("start", 0.0)) + offset_sec
                                    w_copy["end"] = float(w_copy.get("end", w_copy.get("start", 0.0))) + offset_sec
                                    words.append(w_copy)
                                seg_copy["words"] = words

                            merged_segments.append(seg_copy)

                    # ë‹¤ìŒ chunkë¡œ ì´ë™
                    start_frame += this_frames
                    offset_sec += this_frames / float(sample_rate)

                if not chunk_results:
                    raise RuntimeError("chunk STT ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

                merged_text = " ".join(text_parts).strip()
                merged_language = chunk_results[0].get("language", "unknown")

                result = {
                    "text": merged_text,
                    "segments": merged_segments,
                    "language": merged_language,
                    "duration": total_duration or (offset_sec if offset_sec > 0 else None),
                }

            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            transcribed_text = result["text"].strip()
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            self._save_transcript_with_timestamps(audio_file, output_text_file, result)
            
            # ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ íŒŒì¼ ìƒì„± (ì˜µì…˜ì— ë”°ë¼)
            simple_file = Path(output_text_file).parent / f"{Path(output_text_file).stem}_simple.txt"
            if enable_timestamps and diarization_audio_file:
                # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í™”ìë¶„ë¦¬ ëª¨ë‘ í™œì„±í™”
                self._save_simple_transcript(simple_file, result, diarization_audio_file)
                logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„+í™”ìë¶„ë¦¬): {simple_file}")
            elif enable_timestamps:
                # íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ í™œì„±í™”
                self._save_simple_transcript_timestamps_only(simple_file, result)
                logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ): {simple_file}")
            elif diarization_audio_file:
                # í™”ìë¶„ë¦¬ë§Œ í™œì„±í™”
                self._save_simple_transcript_speakers_only(simple_file, result, diarization_audio_file)
                logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„± (í™”ìë¶„ë¦¬ë§Œ): {simple_file}")
            else:
                # ë‘˜ ë‹¤ ë¹„í™œì„±í™” - ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ
                self._save_simple_transcript_text_only(simple_file, result)
                logger.info(f"ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„± (í…ìŠ¤íŠ¸ë§Œ): {simple_file}")
            
            # SRT ìë§‰ íŒŒì¼ ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
            if srt_file:
                diarization_source = diarization_audio_file or audio_file
                self._save_srt_file(srt_file, result, diarization_source)
                logger.info(f"SRT ìë§‰ íŒŒì¼ ìƒì„±: {srt_file}")
            
            logger.info(f"STT ì²˜ë¦¬ ì™„ë£Œ: {output_text_file}")
            logger.info(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {transcribed_text[:100]}...")
            
            return transcribed_text
            
        except Exception as e:
            logger.error(f"STT ì²˜ë¦¬ ì‹¤íŒ¨ ({audio_file}): {e}")
            raise
    
    def _save_transcript_with_timestamps(self, audio_file, output_text_file, result):
        """íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ì „ì‚¬ ê²°ê³¼ ì €ì¥"""
        with open(output_text_file, 'w', encoding='utf-8') as f:
            # í—¤ë” ì •ë³´
            f.write(f"íŒŒì¼ëª…: {Path(audio_file).name}\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì–¸ì–´: {result.get('language', 'unknown')}\n")
            f.write(f"ì´ ê¸¸ì´: {self._format_time(result.get('duration', 0))}\n")
            f.write("=" * 60 + "\n\n")
            
            # ì „ì²´ í…ìŠ¤íŠ¸
            f.write("ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            f.write(result["text"].strip() + "\n\n")
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
            f.write("â° íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í…ìŠ¤íŠ¸\n")
            f.write("-" * 30 + "\n")
            
            if "segments" in result:
                for i, segment in enumerate(result["segments"], 1):
                    start_time = self._format_time(segment["start"])
                    end_time = self._format_time(segment["end"])
                    text = segment["text"].strip()
                    
                    f.write(f"[{start_time} â†’ {end_time}] {text}\n")
                
                # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ (ê°€ëŠ¥í•œ ê²½ìš°)
                f.write("\nğŸ”¤ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„\n")
                f.write("-" * 30 + "\n")
                
                for segment in result["segments"]:
                    if "words" in segment:
                        for word_info in segment["words"]:
                            start_time = self._format_time(word_info["start"])
                            end_time = self._format_time(word_info["end"])
                            word = word_info["word"].strip()
                            confidence = word_info.get("probability", 0)
                            
                            f.write(f"[{start_time}-{end_time}] {word} (ì‹ ë¢°ë„: {confidence:.2f})\n")
            else:
                f.write("íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")
    
    def _format_time(self, seconds):
        """ì´ˆë¥¼ MM:SS.mmm í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00.000"
        
        minutes = int(seconds // 60)
        seconds = seconds % 60
        return f"{minutes:02d}:{seconds:06.3f}"
    
    def _save_srt_file(self, srt_file, result, audio_file=None):
        """SRT ìë§‰ íŒŒì¼ ìƒì„± (í™”ì ì •ë³´ í¬í•¨)"""
        with open(srt_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‚¬ìš©
                if audio_file:
                    # í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
                    self._current_audio_file = audio_file
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                    # ì„ì‹œ ì •ë³´ ì œê±°
                    delattr(self, '_current_audio_file')
                else:
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                subtitle_index = 1
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_srt_time(segment["start"])
                    end_time = self._format_srt_time(segment["end"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    f.write(f"{subtitle_index}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{speaker_name}: {text}\n\n")
                    subtitle_index += 1
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ìë§‰ìœ¼ë¡œ
                duration = result.get("duration", 60)  # ê¸°ë³¸ 60ì´ˆ
                start_time = self._format_srt_time(0)
                end_time = self._format_srt_time(duration)
                text = result["text"].strip()
                
                f.write("1\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"í™”ìA: {text}\n\n")
    
    def _format_srt_time(self, seconds):
        """ì´ˆë¥¼ SRT í˜•ì‹ (HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜"""
        if seconds is None:
            return "00:00:00,000"
        
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        milliseconds = int((secs % 1) * 1000)
        secs = int(secs)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    def _save_simple_transcript(self, simple_file, result, audio_file=None):
        """ê°„ë‹¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‚¬ìš©
                if audio_file:
                    logger.info("ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‹œì‘...")
                    # í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
                    self._current_audio_file = audio_file
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                    # ì„ì‹œ ì •ë³´ ì œê±°
                    delattr(self, '_current_audio_file')
                else:
                    logger.info("ëŒ€ì•ˆ í™”ì í• ë‹¹ ì‚¬ìš©...")
                    speaker_assignments = self._assign_smart_speakers(result["segments"])
                
                for i, segment in enumerate(result["segments"]):
                    start_time = self._format_time(segment["start"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    # [ì‹œê°„] í™”ì: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    f.write(f"[{start_time}] {speaker_name}: {text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"[00:00.000] í™”ìA: {result['text'].strip()}\n")
    
    def _save_simple_transcript_timestamps_only(self, simple_file, result):
        """íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ í¬í•¨ëœ ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                for segment in result["segments"]:
                    start_time = self._format_time(segment["start"])
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # [ì‹œê°„] í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (í™”ì ì •ë³´ ì—†ìŒ)
                    f.write(f"[{start_time}] {text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"[00:00.000] {result['text'].strip()}\n")
    
    def _save_simple_transcript_speakers_only(self, simple_file, result, audio_file):
        """í™”ìë¶„ë¦¬ë§Œ í¬í•¨ëœ ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‚¬ìš©
                logger.info("ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‹œì‘...")
                # í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´ë¥¼ ì„ì‹œ ì €ì¥
                self._current_audio_file = audio_file
                speaker_assignments = self._assign_smart_speakers(result["segments"])
                # ì„ì‹œ ì •ë³´ ì œê±°
                delattr(self, '_current_audio_file')
                
                for i, segment in enumerate(result["segments"]):
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # í• ë‹¹ëœ í™”ì ì‚¬ìš©
                    speaker_name = speaker_assignments[i]
                    
                    # í™”ì: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ)
                    f.write(f"{speaker_name}: {text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"í™”ìA: {result['text'].strip()}\n")
    
    def _save_simple_transcript_text_only(self, simple_file, result):
        """ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ í¬í•¨ëœ ê°„ë‹¨í•œ ì „ì‚¬ íŒŒì¼ ìƒì„±"""
        with open(simple_file, 'w', encoding='utf-8') as f:
            if "segments" in result:
                for segment in result["segments"]:
                    text = segment["text"].strip()
                    
                    if not text:
                        continue
                    
                    # ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„, í™”ì ì •ë³´ ì—†ìŒ)
                    f.write(f"{text}\n")
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                f.write(f"{result['text'].strip()}\n")
    
    def _assign_smart_speakers(self, segments):
        """í™”ì ë¶„ë¦¬: WhisperX + pyannoteë¥¼ ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ìŒì„± íŠ¹ì„±/ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©"""
        if not segments:
            return []
        
        logger.info(f"í™”ì ë¶„ë¦¬ ì‹œì‘ - ì´ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë‹¨ì¼ í™”ì
        if len(segments) <= 1:
            logger.info("ì„¸ê·¸ë¨¼íŠ¸ 1ê°œ ì´í•˜ - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
            return ["í™”ìA" for _ in segments]

        # 1ìˆœìœ„: WhisperX + pyannote diarization ì‚¬ìš©
        audio_file = getattr(self, '_current_audio_file', None)
        if (
            self.use_whisperx_diarization
            and WHISPERX_AVAILABLE
            and self.pyannote_auth_token
            and audio_file
        ):
            try:
                logger.info("WhisperX + pyannote ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì‹œë„")
                return self._assign_speakers_with_whisperx(audio_file, segments)
            except Exception as e:
                logger.error(f"WhisperX í™”ì ë¶„ë¦¬ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´: {e}")

        # 2ìˆœìœ„: ê¸°ì¡´ ìŒì„± íŠ¹ì„± ê¸°ë°˜ ë°©ì‹
        if audio_file:
            return self._voice_feature_based_assignment(audio_file, segments)
        else:
            # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ë¡œì§
            return self._fallback_speaker_assignment(segments)

    def _assign_speakers_with_whisperx(self, audio_file, segments):
        """WhisperX + pyannote diarization ê²°ê³¼ë¥¼ Whisper ì„¸ê·¸ë¨¼íŠ¸ì— ë§¤í•‘"""
        if not WHISPERX_AVAILABLE:
            raise RuntimeError("WhisperXê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        if not self.pyannote_auth_token:
            raise RuntimeError("PYANNOTE_AUTH_TOKENì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        logger.info("WhisperX diarization íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        device = self.device
        
        diarize_model = whisperx.DiarizationPipeline(
            use_auth_token=self.pyannote_auth_token,
            device=device,
        )
        
        logger.info("WhisperX diarization ì‹¤í–‰")
        diarize_result = diarize_model(audio_file)
        
        # pyannote Annotation ê°ì²´ì—ì„œ (start, end, speaker_label) ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        diarization_segments = []
        for turn, _, speaker in diarize_result.itertracks(yield_label=True):
            diarization_segments.append(
                {
                    "start": float(turn.start),
                    "end": float(turn.end),
                    "speaker": str(speaker),
                }
            )
        
        if not diarization_segments:
            logger.warning("WhisperX diarization ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
            return self._voice_feature_based_assignment(audio_file, segments)
        
        # diarization speaker ë¼ë²¨ì„ í™”ìA/B/... ë¡œ ë§¤í•‘
        speaker_label_map = {}
        speaker_order = []
        
        def _map_label(raw_label):
            if raw_label not in speaker_label_map:
                speaker_order.append(raw_label)
                idx = len(speaker_order) - 1
                speaker_label_map[raw_label] = f"í™”ì{chr(ord('A') + idx)}"
            return speaker_label_map[raw_label]
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ì‹¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ diarization ì„¸ê·¸ë¨¼íŠ¸ì™€ ë§¤ì¹­
        speaker_assignments = []
        for seg in segments:
            start = float(seg.get("start", 0.0))
            end = float(seg.get("end", start + 0.1))
            mid = (start + end) / 2.0
            
            matched_speaker = None
            for dseg in diarization_segments:
                if dseg["start"] <= mid <= dseg["end"]:
                    matched_speaker = _map_label(dseg["speaker"])
                    break
            
            # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì¸ì ‘í•œ diarization ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ìš©
            if matched_speaker is None:
                best_dist = None
                best_label = None
                for dseg in diarization_segments:
                    if mid < dseg["start"]:
                        dist = dseg["start"] - mid
                    elif mid > dseg["end"]:
                        dist = mid - dseg["end"]
                    else:
                        dist = 0.0
                    if best_dist is None or dist < best_dist:
                        best_dist = dist
                        best_label = dseg["speaker"]
                if best_label is not None:
                    matched_speaker = _map_label(best_label)

            # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ í™”ìA
            if matched_speaker is None:
                matched_speaker = "í™”ìA"

            speaker_assignments.append(matched_speaker)

        from collections import Counter
        logger.info(f"WhisperX ê¸°ë°˜ í™”ì ë¶„í¬: {dict(Counter(speaker_assignments))}")
        speaker_assignments = self._smooth_whisperx_speakers(segments, speaker_assignments)

        return speaker_assignments
    
    def _smooth_whisperx_speakers(self, segments, speaker_assignments):
        try:
            from collections import Counter

            if not segments or not speaker_assignments:
                return speaker_assignments

            n = len(segments)
            assignments = list(speaker_assignments)

            for i, (seg, spk) in enumerate(zip(segments, assignments)):
                start = float(seg.get("start", 0.0))
                end = float(seg.get("end", start))
                duration = max(0.0, end - start)

                prev_spk = assignments[i - 1] if i > 0 else None
                next_spk = assignments[i + 1] if i < n - 1 else None

                if (
                    duration < 0.7
                    and prev_spk is not None
                    and next_spk is not None
                    and prev_spk == next_spk
                    and spk != prev_spk
                ):
                    assignments[i] = prev_spk

            counts = Counter(assignments)
            total = sum(counts.values())

            if total == 0:
                return assignments

            main_speakers = {s for s, c in counts.items() if c / total >= 0.15}

            if not main_speakers:
                return assignments

            for i, spk in enumerate(assignments):
                ratio = counts[spk] / total
                if ratio >= 0.05:
                    continue

                neighbors = []
                if i > 0:
                    neighbors.append(assignments[i - 1])
                if i < n - 1:
                    neighbors.append(assignments[i + 1])

                candidates = [s for s in neighbors if s in main_speakers]
                if candidates:
                    assignments[i] = candidates[0]

            return assignments

        except Exception:
            return speaker_assignments
    
    def _voice_feature_based_assignment(self, audio_file, segments):
        """ìŒì„± íŠ¹ì„±(ì£¼íŒŒìˆ˜, í”¼ì¹˜, ìŠ¤í™íŠ¸ëŸ¼) ê¸°ë°˜ í™”ì ë¶„ë¦¬ + ë…ë°± ì²˜ë¦¬"""
        try:
            logger.info("ìŒì„± íŠ¹ì„± ì¶”ì¶œ ë° ë…ë°± ë¶„ì„ ì¤‘...")
            
            # ë…ë°± ì—¬ë¶€ ì‚¬ì „ íŒë‹¨
            is_monologue = self._detect_monologue_pattern(segments)
            if is_monologue:
                logger.info("ë…ë°± íŒ¨í„´ ê°ì§€ - ë…ë°± ì „ìš© ì²˜ë¦¬ ëª¨ë“œ")
                return self._handle_monologue_segments(segments)
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(audio_file)
            
            # ëª¨ë…¸ ì±„ë„ë¡œ ë³€í™˜
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ì„¸ê·¸ë¨¼íŠ¸ë³„ ìŒì„± íŠ¹ì„± ì¶”ì¶œ
            voice_features = []
            valid_segments = []
            
            for i, segment in enumerate(segments):
                start_time = segment.get("start", 0)
                end_time = segment.get("end", start_time + 1)
                text = segment.get("text", "").strip()
                
                if not text or end_time <= start_time:
                    continue
                
                # ì„¸ê·¸ë¨¼íŠ¸ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                start_sample = int(start_time * sample_rate)
                end_sample = int(end_time * sample_rate)
                
                if start_sample >= waveform.shape[1] or end_sample <= start_sample:
                    continue
                
                segment_audio = waveform[:, start_sample:end_sample]
                
                # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê±´ë„ˆë›°ê¸° (ìµœì†Œ 0.3ì´ˆ)
                if segment_audio.shape[1] < sample_rate * 0.3:
                    continue
                
                # ìŒì„± íŠ¹ì„± ì¶”ì¶œ
                features = self._extract_voice_features(segment_audio.squeeze(), sample_rate)
                if features is not None:
                    voice_features.append(features)
                    valid_segments.append((i, segment))
            
            if len(voice_features) < 2:
                logger.info("ìœ íš¨í•œ ìŒì„± íŠ¹ì„±ì´ ë¶€ì¡± - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
                return ["í™”ìA" for _ in segments]
            
            # ìŒì„± íŠ¹ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
            speaker_labels = self._cluster_voice_features(voice_features)
            
            # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
            return self._assign_speakers_from_clusters(segments, valid_segments, speaker_labels)
            
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return self._fallback_speaker_assignment(segments)
    
    def _detect_monologue_pattern(self, segments):
        """ë…ë°± íŒ¨í„´ ê°ì§€ (ê°œì„ ëœ ë²„ì „)"""
        try:
            logger.info(f"ë…ë°± íŒ¨í„´ ë¶„ì„ ì¤‘... (ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(segments)})")
            
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ 1ê°œì¸ ê²½ìš°ì—ë§Œ í™•ì‹¤í•œ ë…ë°±ìœ¼ë¡œ ì²˜ë¦¬
            if len(segments) <= 1:
                logger.info("ì„¸ê·¸ë¨¼íŠ¸ 1ê°œ ì´í•˜ - ë…ë°±ìœ¼ë¡œ íŒë‹¨")
                return True
            
            # 1. ì¹¨ë¬µ ì‹œê°„ ë¶„ì„
            silence_durations = []
            long_silences = 0  # 2ì´ˆ ì´ìƒ ì¹¨ë¬µ (ê¸°ì¤€ ì™„í™”)
            very_long_silences = 0  # 5ì´ˆ ì´ìƒ ì¹¨ë¬µ
            
            for i in range(1, len(segments)):
                prev_end = segments[i-1].get("end", 0)
                curr_start = segments[i].get("start", 0)
                silence = curr_start - prev_end
                silence_durations.append(silence)
                
                if silence > 5.0:
                    very_long_silences += 1
                elif silence > 2.0:
                    long_silences += 1
            
            avg_silence = sum(silence_durations) / len(silence_durations) if silence_durations else 0
            
            # 2. ë°œí™” ê¸¸ì´ ë¶„ì„
            segment_durations = []
            for segment in segments:
                duration = segment.get("end", 0) - segment.get("start", 0)
                segment_durations.append(duration)
            
            avg_duration = sum(segment_durations) / len(segment_durations)
            max_duration = max(segment_durations) if segment_durations else 0
            
            # 3. í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„
            total_text_length = sum(len(segment.get("text", "")) for segment in segments)
            avg_text_length = total_text_length / len(segments)
            
            # 4. í™”ì ë³€ê²½ ì‹ í˜¸ ë¶„ì„
            speaker_change_signals = 0
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip()
                prev_text = segments[i-1].get("text", "").strip()
                
                # ëŒ€í™” ì‹ í˜¸ í‚¤ì›Œë“œ (ì§ˆë¬¸-ì‘ë‹µ íŒ¨í„´)
                question_words = ['?', 'ï¼Ÿ', 'ë­', 'ë¬´ì—‡', 'ãªã«', 'what', 'how']
                response_words = ['ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆ', 'ã¯ã„', 'ãã†', 'yes', 'no']
                
                if (any(q in prev_text for q in question_words) and 
                    any(r in curr_text for r in response_words)):
                    speaker_change_signals += 1
            
            # 5. ë…ë°± íŒë‹¨ ê¸°ì¤€ (í›¨ì”¬ ì—„ê²©í•˜ê²Œ)
            #    â†’ ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì˜¤ê°€ê³ , ì§ˆë¬¸/ì‘ë‹µ íŒ¨í„´ì´ ì¡°ê¸ˆì´ë¼ë„ ë³´ì´ë©´ ëŒ€í™”ë¡œ ì²˜ë¦¬
            monologue_indicators = [
                very_long_silences == 0,               # ë§¤ìš° ê¸´ ì¹¨ë¬µ(5ì´ˆ+)ì´ ì—†ìŒ
                avg_silence < 1.0,                     # í‰ê·  ì¹¨ë¬µì´ 1ì´ˆ ë¯¸ë§Œìœ¼ë¡œ ë§¤ìš° ì´˜ì´˜í•˜ê²Œ ì´ì–´ì§
                avg_duration > 3.0 or max_duration > 8.0,  # ë°œí™”ê°€ ê¸¸ê³  ì„¤ëª… ìœ„ì£¼ì¸ ê²½ìš°
                avg_text_length > 40,                  # í‰ê·  í…ìŠ¤íŠ¸ê°€ ìƒë‹¹íˆ ê¸¸ ë•Œë§Œ
                speaker_change_signals == 0,           # ì§ˆë¬¸-ì‘ë‹µ íŒ¨í„´ì´ ì „í˜€ ì—†ì„ ë•Œë§Œ
                len(segments) <= 3                     # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì•„ì£¼ ì ì„ ë•Œë§Œ
            ]

            monologue_score = sum(monologue_indicators)

            logger.info(
                f"ë…ë°± ë¶„ì„ ê²°ê³¼: ì ìˆ˜ {monologue_score}/6 "
                f"(ë§¤ìš°ê¸´ì¹¨ë¬µ: {very_long_silences}, ê¸´ì¹¨ë¬µ: {long_silences}, "
                f"í‰ê· ì¹¨ë¬µ: {avg_silence:.1f}ì´ˆ, í‰ê· ë°œí™”: {avg_duration:.1f}ì´ˆ, "
                f"ìµœëŒ€ë°œí™”: {max_duration:.1f}ì´ˆ, í‰ê· í…ìŠ¤íŠ¸: {avg_text_length:.1f}ì, "
                f"í™”ìë³€ê²½ì‹ í˜¸: {speaker_change_signals})"
            )

            # ì´ì œëŠ” 6ê°œ ì¤‘ 5ê°œ ì´ìƒ ë§Œì¡±í•  ë•Œë§Œ ë…ë°±ìœ¼ë¡œ ë³¸ë‹¤
            is_monologue = monologue_score >= 5

            if is_monologue:
                logger.info("ğŸ¤ ë…ë°±ìœ¼ë¡œ íŒë‹¨ë¨ - ë…ë°± ì „ìš© ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")
            else:
                logger.info("ğŸ’¬ ëŒ€í™”ë¡œ íŒë‹¨ë¨ - ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ìë¶„ë¦¬ ì§„í–‰")

            return is_monologue
            
        except Exception as e:
            logger.error(f"ë…ë°± íŒ¨í„´ ê°ì§€ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë…ë°±ìœ¼ë¡œ ì²˜ë¦¬ (ë‹¨ì¼ í™”ì ìš°ì„ )
            logger.info("ì˜¤ë¥˜ë¡œ ì¸í•´ ë…ë°±ìœ¼ë¡œ ì²˜ë¦¬")
            return True
    
    def _handle_monologue_segments(self, segments):
        """ë…ë°± ì„¸ê·¸ë¨¼íŠ¸ ì „ìš© ì²˜ë¦¬ - ë‹¨ì¼ í™”ì ìœ ì§€"""
        try:
            logger.info("ğŸ¤ ë…ë°± ëª¨ë“œ: ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
            
            # ë…ë°±ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¨ì¼ í™”ì (í™”ìA)
            speaker_assignments = ["í™”ìA" for _ in segments]
            
            # ë…ë°± ë‚´ì—ì„œë„ ëª…í™•í•œ ì£¼ì œ ì „í™˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í™”ì ë¶„ë¦¬
            topic_changes = self._detect_strong_topic_changes(segments)
            
            if topic_changes:
                logger.info(f"ë…ë°± ë‚´ ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€: {len(topic_changes)}ê°œ ì§€ì ")
                current_speaker = 'A'
                
                for i, segment in enumerate(segments):
                    if i in topic_changes:
                        current_speaker = 'B' if current_speaker == 'A' else 'A'
                        logger.info(f"ì„¸ê·¸ë¨¼íŠ¸ {i}: ê°•í•œ ì£¼ì œ ì „í™˜ â†’ í™”ì{current_speaker}")
                    
                    speaker_assignments[i] = f"í™”ì{current_speaker}"
            else:
                logger.info("ì£¼ì œ ì „í™˜ ì—†ìŒ - ì™„ì „í•œ ë‹¨ì¼ í™”ì ìœ ì§€")
            
            # ë…ë°± ê²°ê³¼ ë¡œê¹…
            from collections import Counter
            speaker_count = Counter(speaker_assignments)
            logger.info(f"ğŸ¤ ë…ë°± ì²˜ë¦¬ ì™„ë£Œ: {dict(speaker_count)}")
            
            return speaker_assignments
            
        except Exception as e:
            logger.error(f"ë…ë°± ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return ["í™”ìA" for _ in segments]
    
    def _detect_strong_topic_changes(self, segments):
        """ë…ë°± ë‚´ ê°•í•œ ì£¼ì œ ì „í™˜ë§Œ ê°ì§€ (ë§¤ìš° ì—„ê²©í•œ ê¸°ì¤€)"""
        try:
            topic_changes = []
            
            # ë§¤ìš° ê°•í•œ ì£¼ì œ ì „í™˜ ì‹ í˜¸ë§Œ ê°ì§€
            strong_transition_keywords = [
                # í•œêµ­ì–´ - ëª…í™•í•œ ì „í™˜
                'ê·¸ëŸ°ë° ë§ì´ì•¼', 'ì•„ ê·¸ë¦¬ê³ ', 'ì°¸ ê·¸ëŸ°ë°', 'ì•„ ë§ë‹¤', 'ê·¸ê±´ ê·¸ë ‡ê³ ',
                # ì¼ë³¸ì–´ - ëª…í™•í•œ ì „í™˜  
                'ã¨ã“ã‚ã§', 'ãã†ã„ãˆã°', 'ã‚ã€ãã†ãã†', 'ãã‚Œã¯ãã†ã¨',
                # ì˜ì–´ - ëª…í™•í•œ ì „í™˜
                'by the way', 'speaking of', 'oh and', 'that reminds me'
            ]
            
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip().lower()
                
                # ê°•í•œ ì „í™˜ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                if any(keyword in curr_text for keyword in strong_transition_keywords):
                    topic_changes.append(i)
                    logger.debug(f"ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i}")
            
            logger.info(f"ê°•í•œ ì£¼ì œ ì „í™˜ ì§€ì : {len(topic_changes)}ê°œ")
            return topic_changes
            
        except Exception as e:
            logger.error(f"ê°•í•œ ì£¼ì œ ì „í™˜ ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _detect_topic_changes(self, segments):
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì£¼ì œ ë³€í™” ê°ì§€"""
        try:
            topic_changes = []
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ë³€í™” ê°ì§€
            for i in range(1, len(segments)):
                curr_text = segments[i].get("text", "").strip()
                prev_text = segments[i-1].get("text", "").strip()
                
                # ì£¼ì œ ë³€í™” ì‹ í˜¸ í‚¤ì›Œë“œ (í•œêµ­ì–´, ì¼ë³¸ì–´, ì˜ì–´)
                topic_change_keywords = [
                    # í•œêµ­ì–´
                    'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•œí¸', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê²°êµ­', 'ë§ˆì§€ë§‰ìœ¼ë¡œ',
                    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë‹¤ìŒìœ¼ë¡œ', 'ì´ì œ', 'ê·¸ëŸ¼', 'ê·¸ëŸ¬ë©´',
                    # ì¼ë³¸ì–´  
                    'ãã‚Œã§', 'ãã—ã¦', 'ã¾ãŸ', 'ã—ã‹ã—', 'ã§ã‚‚', 'ã¨ã“ã‚ã§', 'ã•ã¦',
                    'ã¾ãš', 'æ¬¡ã«', 'æœ€å¾Œã«', 'çµå±€', 'ã¤ã¾ã‚Š', 'ã ã‹ã‚‰',
                    # ì˜ì–´
                    'however', 'but', 'and', 'also', 'then', 'next', 'finally',
                    'first', 'second', 'third', 'so', 'therefore', 'meanwhile'
                ]
                
                # í˜„ì¬ í…ìŠ¤íŠ¸ì— ì£¼ì œ ë³€í™” í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                curr_lower = curr_text.lower()
                if any(keyword in curr_lower for keyword in topic_change_keywords):
                    topic_changes.append(i)
                    logger.debug(f"ì£¼ì œ ë³€í™” ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i} - {curr_text[:30]}...")
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸‰ë³€ (ê¸´ ì„¤ëª… í›„ ì§§ì€ ìš”ì•½ ë“±)
                if len(prev_text) > 50 and len(curr_text) < 20:
                    topic_changes.append(i)
                    logger.debug(f"í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸‰ë³€ ê°ì§€: ì„¸ê·¸ë¨¼íŠ¸ {i}")
            
            logger.info(f"ì£¼ì œ ë³€í™” ì§€ì : {len(topic_changes)}ê°œ - {topic_changes}")
            return topic_changes
            
        except Exception as e:
            logger.error(f"ì£¼ì œ ë³€í™” ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_voice_features(self, audio_segment, sample_rate):
        """ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ (í”¼ì¹˜, ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬, MFCC)"""
        try:
            import librosa
            import numpy as np
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            if isinstance(audio_segment, torch.Tensor):
                audio_np = audio_segment.numpy()
            else:
                audio_np = audio_segment
            
            # 1. ê¸°ë³¸ ì£¼íŒŒìˆ˜ (F0) - í”¼ì¹˜
            f0 = librosa.yin(audio_np, fmin=50, fmax=400, sr=sample_rate)
            f0_mean = np.nanmean(f0[f0 > 0]) if np.any(f0 > 0) else 150
            f0_std = np.nanstd(f0[f0 > 0]) if np.any(f0 > 0) else 0
            
            # 2. MFCC (Mel-frequency cepstral coefficients) - ìŒìƒ‰ íŠ¹ì„±
            mfcc = librosa.feature.mfcc(y=audio_np, sr=sample_rate, n_mfcc=13)
            mfcc_mean = np.mean(mfcc, axis=1)
            mfcc_std = np.std(mfcc, axis=1)
            
            # 3. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (Spectral Centroid) - ìŒì„±ì˜ ë°ê¸°
            spectral_centroid = librosa.feature.spectral_centroid(y=audio_np, sr=sample_rate)
            sc_mean = np.mean(spectral_centroid)
            sc_std = np.std(spectral_centroid)
            
            # 4. ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­ (Spectral Bandwidth)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_np, sr=sample_rate)
            sb_mean = np.mean(spectral_bandwidth)
            
            # 5. ì˜êµì°¨ìœ¨ (Zero Crossing Rate) - ìŒì„±ì˜ ê±°ì¹ ê¸°
            zcr = librosa.feature.zero_crossing_rate(audio_np)
            zcr_mean = np.mean(zcr)
            
            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = np.concatenate([
                [f0_mean, f0_std],           # í”¼ì¹˜ íŠ¹ì„± (2ì°¨ì›)
                mfcc_mean[:8],               # MFCC í‰ê·  (8ì°¨ì›)
                [sc_mean, sc_std],           # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (2ì°¨ì›)
                [sb_mean],                   # ìŠ¤í™íŠ¸ëŸ´ ëŒ€ì—­í­ (1ì°¨ì›)
                [zcr_mean]                   # ì˜êµì°¨ìœ¨ (1ì°¨ì›)
            ])
            
            # NaN ê°’ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0)
            
            logger.debug(f"ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: F0={f0_mean:.1f}Hz, SC={sc_mean:.1f}Hz")
            return features
            
        except ImportError:
            logger.warning("librosaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ íŠ¹ì„±ë§Œ ì¶”ì¶œ")
            return self._extract_simple_voice_features(audio_segment, sample_rate)
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_simple_voice_features(self, audio_segment, sample_rate):
        """librosa ì—†ì´ ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ"""
        try:
            import numpy as np
            
            if isinstance(audio_segment, torch.Tensor):
                audio_np = audio_segment.numpy()
            else:
                audio_np = audio_segment
            
            # 1. RMS ì—ë„ˆì§€ (ìŒëŸ‰)
            rms_energy = np.sqrt(np.mean(audio_np**2))
            
            # 2. ì˜êµì°¨ìœ¨ (ìŒì„±ì˜ ê±°ì¹ ê¸°)
            zero_crossings = np.sum(np.diff(np.sign(audio_np)) != 0)
            zcr = zero_crossings / len(audio_np)
            
            # 3. ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ (FFT)
            fft = np.fft.fft(audio_np)
            magnitude = np.abs(fft[:len(fft)//2])
            freqs = np.fft.fftfreq(len(audio_np), 1/sample_rate)[:len(fft)//2]
            
            # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (ê°€ì¤‘ í‰ê·  ì£¼íŒŒìˆ˜)
            if np.sum(magnitude) > 0:
                spectral_centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
            else:
                spectral_centroid = 0
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ (ìµœëŒ€ ì—ë„ˆì§€ ì£¼íŒŒìˆ˜)
            dominant_freq = freqs[np.argmax(magnitude)] if len(magnitude) > 0 else 0
            
            # íŠ¹ì„± ë²¡í„° êµ¬ì„±
            features = np.array([
                rms_energy,
                zcr,
                spectral_centroid,
                dominant_freq,
                np.mean(magnitude),
                np.std(magnitude)
            ])
            
            # NaN ê°’ ì²˜ë¦¬
            features = np.nan_to_num(features, nan=0.0)
            
            logger.debug(f"ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ: ì£¼íŒŒìˆ˜={dominant_freq:.1f}Hz, ì—ë„ˆì§€={rms_energy:.3f}")
            return features
        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _cluster_voice_features(self, voice_features):
        """ìŒì„± íŠ¹ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            import numpy as np
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler

            features_array = np.array(voice_features)

            # íŠ¹ì„± ì •ê·œí™”
            scaler = StandardScaler()
            features_normalized = scaler.fit_transform(features_array)

            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
            n_segments = len(voice_features)
            if n_segments <= 2:
                n_clusters = 1  # ë‹¨ì¼ í™”ì
            elif n_segments <= 4:
                n_clusters = 2  # 2ëª… í™”ì
            elif n_segments <= 7:
                n_clusters = min(2, n_segments - 1)  # ìµœëŒ€ 2ëª…
            else:
                n_clusters = min(3, n_segments // 2)  # ê¸°ë³¸ ìµœëŒ€ 3ëª…

            # ì „ì—­ ì„¤ì •ì— ë”°ë¥¸ ìƒí•œ ì ìš© (1~max_speakers)
            max_speakers = getattr(self, "max_speakers", None)
            if max_speakers is not None and max_speakers > 0:
                n_clusters = max(1, min(n_clusters, int(max_speakers)))

            if n_clusters == 1:
                logger.info("ë‹¨ì¼ í™”ìë¡œ í´ëŸ¬ìŠ¤í„°ë§")
                return [0] * len(voice_features)

            # K-means í´ëŸ¬ìŠ¤í„°ë§ (ì—¬ëŸ¬ ë²ˆ ì‹œë„í•´ì„œ ìµœì  ê²°ê³¼ ì„ íƒ)
            best_labels = None
            best_inertia = float('inf')

            for attempt in range(5):  # 5ë²ˆ ì‹œë„
                try:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42 + attempt, n_init=10)
                    labels = kmeans.fit_predict(features_normalized)

                    if kmeans.inertia_ < best_inertia:
                        best_inertia = kmeans.inertia_
                        best_labels = labels
                        best_centers = kmeans.cluster_centers_
                except Exception:
                    continue

            if best_labels is None:
                logger.warning("í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ - ë‹¨ì¼ í™”ìë¡œ ì²˜ë¦¬")
                return [0] * len(voice_features)

            logger.info(f"ìŒì„± íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ëª… í™”ì ê°ì§€ (ê´€ì„±: {best_inertia:.2f})")

            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê²€ì¦
            unique_labels = len(set(best_labels))
            if unique_labels < n_clusters:
                logger.warning(f"ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ê°€ ë¹„ì–´ìˆìŒ: {unique_labels}/{n_clusters}")

            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ì •ë³´ ë¡œê¹… (ê°„ë‹¨ ìš”ì•½)
            for i, center in enumerate(best_centers):
                logger.debug(f"í™”ì{chr(65 + i)} íŠ¹ì„± ìš”ì•½: F0={center[0]:.1f}, MFCC1={center[2]:.2f}")

            return best_labels

        except ImportError:
            logger.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê°„ë‹¨í•œ ë¶„ë¥˜ ì‚¬ìš©")
            return self._simple_voice_clustering(voice_features)
        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return [0] * len(voice_features)  # ëª¨ë‘ ê°™ì€ í™”ìë¡œ ì²˜ë¦¬

    def _simple_voice_clustering(self, voice_features):
        """ê°„ë‹¨í•œ ìŒì„± íŠ¹ì„± ê¸°ë°˜ ë¶„ë¥˜ (scikit-learn ë¯¸ì‚¬ìš© ì‹œ)"""
        try:
            import numpy as np

            features_array = np.array(voice_features)

            # ì²« ë²ˆì§¸ íŠ¹ì„±(ì˜ˆ: í”¼ì¹˜)ì„ ê¸°ì¤€ìœ¼ë¡œ 2ê·¸ë£¹ ë¶„í• 
            first_feature = features_array[:, 0]
            median_value = np.median(first_feature)
            labels = (first_feature > median_value).astype(int)

            logger.info(f"ê°„ë‹¨í•œ ìŒì„± ë¶„ë¥˜ ì™„ë£Œ: ê¸°ì¤€ê°’={median_value:.2f}")
            return labels

        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ìŒì„± ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return [0] * len(voice_features)

    def _assign_speakers_from_clusters(self, segments, valid_segments, speaker_labels):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í• ë‹¹"""
        speaker_assignments = []
        label_to_speaker = {}

        # ë¼ë²¨ì„ í™”ì ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
        unique_labels = sorted(set(speaker_labels))
        for i, label in enumerate(unique_labels):
            speaker_letter = chr(ord('A') + i)
            label_to_speaker[label] = f"í™”ì{speaker_letter}"

        # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì¸ë±ìŠ¤ì™€ ë¼ë²¨ ë§¤í•‘
        valid_assignments = {}
        for (seg_idx, _segment), label in zip(valid_segments, speaker_labels):
            valid_assignments[seg_idx] = label_to_speaker[label]

        # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹ (ì•ì—ì„œ ì •í•œ í™”ì ìœ ì§€)
        current_speaker = "í™”ìA"
        for i, _segment in enumerate(segments):
            if i in valid_assignments:
                current_speaker = valid_assignments[i]
            speaker_assignments.append(current_speaker)

        # í™”ì ì¼ê´€ì„± í›„ì²˜ë¦¬
        speaker_assignments = self._post_process_voice_consistency(
            speaker_assignments, valid_segments, speaker_labels
        )

        from collections import Counter
        speaker_count = Counter(speaker_assignments)
        logger.info(f"ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ë¶„í¬: {dict(speaker_count)}")

        return speaker_assignments

    def unload_models(self):
        """ë¡œë”©ëœ STT/ë…¸ì´ì¦ˆì œê±°/í™”ìë¶„ë¦¬ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ"""
        try:
            logger.info("ğŸ§¹ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ì–¸ë¡œë“œ ì‹œì‘")

            # Whisper STT ëª¨ë¸ í•´ì œ
            if self.whisper_model is not None:
                del self.whisper_model
                self.whisper_model = None

            # ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ í•´ì œ
            if self.denoiser not in (None, "simple_filter"):
                try:
                    del self.denoiser
                except Exception:
                    pass
                self.denoiser = None

            # í™”ì ì¸ì½”ë” ëª¨ë¸ í•´ì œ
            if self.speaker_encoder is not None:
                try:
                    del self.speaker_encoder
                except Exception:
                    pass
                self.speaker_encoder = None

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.use_gpu and torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("âœ… ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _post_process_voice_consistency(self, speaker_assignments, valid_segments, speaker_labels):
        """ìŒì„± íŠ¹ì„± ê¸°ë°˜ í™”ì ì¼ê´€ì„± í›„ì²˜ë¦¬"""
        try:
            from collections import Counter

            logger.info("ìŒì„± íŠ¹ì„± ê¸°ë°˜ ì¼ê´€ì„± í›„ì²˜ë¦¬ ì‹œì‘...")

            speaker_counts = Counter(speaker_assignments)
            logger.info(f"í›„ì²˜ë¦¬ ì „ í™”ì ë¶„í¬: {dict(speaker_counts)}")

            # í™”ì ìˆ˜ê°€ ì´ë¯¸ ì ìœ¼ë©´(<= max_speakers) ë³´ìˆ˜ì ìœ¼ë¡œ ìœ ì§€
            max_speakers = getattr(self, "max_speakers", None)
            if max_speakers is not None and max_speakers > 0:
                if len(speaker_counts) <= max_speakers:
                    logger.info("í™”ì ìˆ˜ê°€ ì„¤ì • ìƒí•œ ì´ë‚´ - í›„ì²˜ë¦¬ ìµœì†Œí™”")
                    return speaker_assignments

            # ê³ ë¦½ëœ í™”ì(ì„¸ê·¸ë¨¼íŠ¸ 1ê°œë§Œ ê°€ì§„ í™”ì)ë§Œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ í†µí•©
            isolated_speakers = [s for s, c in speaker_counts.items() if c == 1]
            if not isolated_speakers:
                logger.info("ê³ ë¦½ëœ í™”ì ì—†ìŒ - í›„ì²˜ë¦¬ ì™„ë£Œ")
                return speaker_assignments

            assignments = list(speaker_assignments)

            for isolated in isolated_speakers:
                idx = assignments.index(isolated)
                prev_spk = assignments[idx - 1] if idx > 0 else None
                next_spk = assignments[idx + 1] if idx < len(assignments) - 1 else None

                target = None
                if prev_spk is not None and prev_spk == next_spk and speaker_counts[prev_spk] >= 3:
                    target = prev_spk

                if target:
                    logger.info(f"ê³ ë¦½ëœ í™”ì {isolated} â†’ {target} ë¡œ í†µí•©")
                    assignments[idx] = target

            final_counts = Counter(assignments)
            logger.info(f"í›„ì²˜ë¦¬ í›„ í™”ì ë¶„í¬: {dict(final_counts)}")
            return assignments

        except Exception as e:
            logger.error(f"ìŒì„± íŠ¹ì„± ì¼ê´€ì„± í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return speaker_assignments

    def _fallback_speaker_assignment(self, segments):
        """ìŒì„± íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ë¡œì§"""
        logger.info("ëŒ€ì•ˆ í™”ì í• ë‹¹ ë¡œì§ ì‚¬ìš©")

        # 1.5ì´ˆ ì´ìƒ ì¹¨ë¬µ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨ ë¶„ë¦¬ (A/B ë²ˆê°ˆì•„ ê°€ë©°)
        speaker_assignments = []
        current_speaker = 'A'

        for i, segment in enumerate(segments):
            if i > 0:
                prev_end = segments[i - 1].get("end", 0)
                curr_start = segment.get("start", 0)
                silence_duration = curr_start - prev_end

                if silence_duration > 1.5:
                    current_speaker = 'B' if current_speaker == 'A' else 'A'

            speaker_assignments.append(f"í™”ì{current_speaker}")

        return speaker_assignments

    def _is_single_speaker(self, segments):
        """ê°„ë‹¨í•œ ë‹¨ì¼ í™”ì íŒë‹¨ ë¡œì§"""
        if len(segments) <= 2:
            logger.info("ì„¸ê·¸ë¨¼íŠ¸ 2ê°œ ì´í•˜ - ë‹¨ì¼ í™”ìë¡œ íŒë‹¨")
            return True

        long_silence_count = 0
        for i in range(1, len(segments)):
            prev_end = segments[i - 1].get("end", 0)
            curr_start = segments[i].get("start", 0)
            silence_duration = curr_start - prev_end

            if silence_duration > 1.5:
                long_silence_count += 1

        is_single = long_silence_count == 0
        logger.info(
            f"ë‹¨ì¼ í™”ì íŒë‹¨: {'ë‹¨ì¼' if is_single else 'ë‹¤ì¤‘'} "
            f"(1.5ì´ˆ+ ì¹¨ë¬µ: {long_silence_count}íšŒ, ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ)"
        )

        return is_single

    def _assign_ecapa_speakers(self, audio_file, segments):
        """ECAPA-VOXCELEB ê¸°ë°˜ ì‹¤ì œ í™”ìë¶„ë¦¬"""
        # í™”ì ì„ë² ë”© ì¶”ì¶œ
        embedding_result = self._extract_speaker_embeddings(audio_file, segments)
        
        if embedding_result is None:
            logger.warning("ECAPA í™”ìë¶„ë¦¬ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©")
            return self._assign_smart_speakers(segments)
        
        embeddings, valid_segments = embedding_result
        
        # í™”ì í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels = self._cluster_speakers(embeddings)
        
        if cluster_labels is None:
            logger.warning("í™”ì í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©")
            return self._assign_smart_speakers(segments)
        
        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ í™”ì ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        unique_labels = sorted(set(cluster_labels))
        label_to_speaker = {}
        
        for i, label in enumerate(unique_labels):
            speaker_letter = chr(ord('A') + i)
            label_to_speaker[label] = f"í™”ì{speaker_letter}"
        
        # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì— í™”ì í• ë‹¹
        speaker_assignments = []
        valid_idx = 0
        
        for segment in segments:
            text = segment.get("text", "").strip()
            if not text:
                # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì´ì „ í™”ì ìœ ì§€
                if speaker_assignments:
                    speaker_assignments.append(speaker_assignments[-1])
                else:
                    speaker_assignments.append("í™”ìA")
                continue
            
            # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ì¸ì§€ í™•ì¸
            if valid_idx < len(valid_segments) and segment == valid_segments[valid_idx]:
                # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‚¬ìš©
                cluster_label = cluster_labels[valid_idx]
                speaker_name = label_to_speaker[cluster_label]
                speaker_assignments.append(speaker_name)
                valid_idx += 1
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì´ì „ í™”ì ìœ ì§€
                if speaker_assignments:
                    speaker_assignments.append(speaker_assignments[-1])
                else:
                    speaker_assignments.append("í™”ìA")
        
        return speaker_assignments
    
    def process_single_file(self, input_file):
        """
        ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
        
        Args:
            input_file (str): ì…ë ¥ ì˜¤ë””ì˜¤ ë˜ëŠ” ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        try:
            input_path = Path(input_file)
            file_stem = input_path.stem
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì—¬ë¶€ í™•ì¸
            if self._is_audio_file(input_file):
                logger.info(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€: {input_file}")
                audio_file = input_file
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ í˜•ì‹: {input_path.suffix}")
            
            # 1ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°
            denoised_file = self.audio_out_dir / f"{file_stem}_denoised.wav"
            self.denoise_audio(audio_file, str(denoised_file))
            
            # 2ë‹¨ê³„: audio_outputìœ¼ë¡œ ë³µì‚¬ (íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ìœ ì§€)
            output_audio_file = self.audio_output_dir / f"{file_stem}_denoised.wav"
            import shutil
            shutil.copy2(denoised_file, output_audio_file)
            
            # 3ë‹¨ê³„: STT ì²˜ë¦¬ (denoised ì˜¤ë””ì˜¤ ì‚¬ìš©, í™”ìë¶„ë¦¬ëŠ” ì›ë³¸ ì˜¤ë””ì˜¤ ì‚¬ìš©)
            transcript_file = self.script_output_dir / f"{file_stem}_transcript.txt"
            srt_file = self.script_output_dir / f"{file_stem}_subtitle.srt"
            transcribed_text = self.transcribe_audio(
                str(output_audio_file),
                str(transcript_file),
                str(srt_file),
                diarization_audio_file=str(audio_file)
            )
            
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {input_file}")
            return transcribed_text
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({input_file}): {e}")
            raise

    def transcribe_uploaded_wav(self, wav_path, save_dir=None, create_srt=True, enable_diarization=True, enable_timestamps=True):
        """ì´ë¯¸ ì¶”ì¶œëœ WAV íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì „ì‚¬ ë° í™”ìë¶„ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°±ì—”ë“œìš© í—¬í¼

        Args:
            wav_path (str): FFmpeg ë“±ìœ¼ë¡œ ë¯¸ë¦¬ ì¶”ì¶œëœ ë‹¨ì¼ ì±„ë„/16kHz WAV íŒŒì¼ ê²½ë¡œ
            save_dir (str, optional): ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬.
                None ì´ë©´ ì…ë ¥ wav íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©.
            create_srt (bool): SRT ìë§‰ íŒŒì¼ ìƒì„± ì—¬ë¶€
            enable_diarization (bool): í™”ìë¶„ë¦¬ í™œì„±í™” ì—¬ë¶€
            enable_timestamps (bool): íƒ€ì„ìŠ¤íƒ¬í”„ í™œì„±í™” ì—¬ë¶€

        Returns:
            dict: {
                "text": ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸ (str),
                "denoised_wav": ë…¸ì´ì¦ˆ ì œê±°ëœ wav ê²½ë¡œ (str),
                "transcript_path": íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì „ì‚¬ txt ê²½ë¡œ (str),
                "simple_path": ê°„ë‹¨ ì „ì‚¬(txt, í™”ì/íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨) ê²½ë¡œ (str),
                "text_only_path": í…ìŠ¤íŠ¸ ì „ìš© íŒŒì¼ ê²½ë¡œ (str),
                "srt_path": SRT ìë§‰ ê²½ë¡œ ë˜ëŠ” None,
            }
        """
        try:
            wav_path = Path(wav_path)
            if not wav_path.exists():
                raise FileNotFoundError(f"ì…ë ¥ WAV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {wav_path}")

            base_dir = Path(save_dir) if save_dir is not None else wav_path.parent
            base_dir.mkdir(parents=True, exist_ok=True)

            file_stem = wav_path.stem

            # 1ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±° (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            denoised_file = base_dir / f"{file_stem}_denoised.wav"
            logger.info(f"[backend] ì—…ë¡œë“œ WAV ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘: {wav_path} -> {denoised_file}")
            self.denoise_audio(str(wav_path), str(denoised_file))

            # 2ë‹¨ê³„: STT + í™”ìë¶„ë¦¬ (STTëŠ” denoised, í™”ìë¶„ë¦¬ëŠ” ì›ë³¸ wav ì‚¬ìš©)
            transcript_file = base_dir / f"{file_stem}_transcript.txt"
            srt_file = base_dir / f"{file_stem}_subtitle.srt" if create_srt else None

            logger.info(f"[backend] ì—…ë¡œë“œ WAV STT ì²˜ë¦¬ ì‹œì‘: {denoised_file}")
            text = self.transcribe_audio(
                str(denoised_file),
                str(transcript_file),
                str(srt_file) if srt_file else None,
                diarization_audio_file=str(wav_path) if enable_diarization else None,
                enable_timestamps=enable_timestamps
            )

            # transcribe_audio ë‚´ë¶€ì—ì„œ simple í…ìŠ¤íŠ¸ë„ ìƒì„±ë¨
            simple_file = base_dir / f"{file_stem}_transcript_simple.txt"

            # ë°±ì—”ë“œì—ì„œ ë°”ë¡œ ë³´ê¸° ì¢‹ì€ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì „ìš© íŒŒì¼ ìƒì„±
            text_only_file = base_dir / f"{file_stem}_text_only.txt"
            try:
                with text_only_file.open("w", encoding="utf-8") as f:
                    f.write(text.strip() + "\n")
                logger.info(f"[backend] í…ìŠ¤íŠ¸ ì „ìš© íŒŒì¼ ìƒì„±: {text_only_file}")
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ ì „ìš© íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

            result = {
                "text": text,
                "denoised_wav": str(denoised_file),
                "transcript_path": str(transcript_file),
                "simple_path": str(simple_file),
                "text_only_path": str(text_only_file),
                "srt_path": str(srt_file) if srt_file else None,
            }

            logger.info(f"[backend] ì—…ë¡œë“œ WAV ì²˜ë¦¬ ì™„ë£Œ: {wav_path}")
            return result

        except Exception as e:
            logger.error(f"ì—…ë¡œë“œ WAV ì „ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({wav_path}): {e}")
            raise
    
    def process_all_files(self):
        """audio_input í´ë”ì˜ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬"""
        # ì§€ì›í•˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¥ì
        file_extensions = ['*.wav', '*.mp3', '*.m4a', '*.flac', '*.ogg', '*.aac']
        
        audio_files = []
        for ext in file_extensions:
            audio_files.extend(glob.glob(str(self.audio_input_dir / ext)))
        
        if not audio_files:
            logger.warning("audio_input í´ë”ì— ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"ì§€ì› í˜•ì‹: {', '.join(file_extensions)}")
            return
        
        logger.info(f"ì´ {len(audio_files)}ê°œ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        for i, media_file in enumerate(audio_files, 1):
            try:
                logger.info(f"[{i}/{len(audio_files)}] ğŸµ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {Path(media_file).name}")
                result = self.process_single_file(media_file)
                results.append({
                    'file': Path(media_file).name,
                    'type': 'audio',
                    'status': 'success',
                    'text': result
                })
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(media_file).name} - {e}")
                results.append({
                    'file': Path(media_file).name,
                    'type': 'audio',
                    'status': 'failed',
                    'error': str(e)
                })
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥
        self._save_processing_summary(results)
        
        logger.info("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
        return results
    
    def _save_processing_summary(self, results):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥"""
        summary_file = self.script_output_dir / f"processing_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=== ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ íŒŒì¼ ìˆ˜: {len(results)}\n")
            
            success_count = sum(1 for r in results if r['status'] == 'success')
            failed_count = len(results) - success_count
            
            f.write(f"ì„±ê³µ: {success_count}ê°œ\n")
            f.write(f"ì‹¤íŒ¨: {failed_count}ê°œ\n")
            f.write("-" * 50 + "\n\n")
            
            for result in results:
                f.write(f"íŒŒì¼: {result['file']}\n")
                f.write(f"ìƒíƒœ: {result['status']}\n")
                if result['status'] == 'success':
                    f.write(f"í…ìŠ¤íŠ¸: {result['text'][:200]}...\n")
                else:
                    f.write(f"ì˜¤ë¥˜: {result['error']}\n")
                f.write("-" * 30 + "\n")
        
        logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì €ì¥: {summary_file}")

def main(target_language=None):
    """
    ë©”ì¸ í•¨ìˆ˜
    
    Args:
        target_language (str): ëŒ€ìƒ ì–¸ì–´ ì½”ë“œ ('ko', 'ja', 'en', etc.)
    """
    print("=== ìŒì„± ë° ë¹„ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
    print("ğŸµ ì˜¤ë””ì˜¤: audio_input â†’ ë…¸ì´ì¦ˆì œê±° â†’ audio_out â†’ STT â†’ script_output")
    print("ğŸ¬ ë¹„ë””ì˜¤: video_input â†’ ì˜¤ë””ì˜¤ì¶”ì¶œ â†’ ë…¸ì´ì¦ˆì œê±° â†’ STT â†’ script_output")
    print()
    
    # ì–¸ì–´ ì„¤ì • ì•ˆë‚´
    if target_language:
        supported_languages = {
            'ko': 'í•œêµ­ì–´', 'ja': 'æ—¥æœ¬èª', 'en': 'English', 'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol', 'fr': 'FranÃ§ais', 'de': 'Deutsch', 'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
        lang_name = supported_languages.get(target_language, target_language)
        print(f"ğŸŒ ëŒ€ìƒ ì–¸ì–´: {lang_name} ({target_language})")
    else:
        print("ğŸŒ ì–¸ì–´: ìë™ ê°ì§€ ëª¨ë“œ")
    print()
    
    try:
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        use_gpu = torch.cuda.is_available()
        if use_gpu:
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = AudioPipeline(use_gpu=use_gpu, target_language=target_language)
        
        # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        results = pipeline.process_all_files()
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            success_count = sum(1 for r in results if r['status'] == 'success')
            print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(results)}ê°œ íŒŒì¼ ì„±ê³µ")
        else:
            print("\nâš ï¸  ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("audio_input í´ë”ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

# ì‚¬ìš© ì˜ˆì‹œ:
# 
# 1. ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬:
#    audio_input/ í´ë”ì— WAV, MP3, M4A, FLAC, OGG íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#
# 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (FFmpeg í•„ìš”):
#    audio_input/ í´ë”ì— MP4, AVI, MOV, MKV, WEBM íŒŒì¼ ì €ì¥ í›„ ì‹¤í–‰
#    
# 3. íŠ¹ì • ì–¸ì–´ ì§€ì •:
#    pipeline = AudioPipeline(target_language='ko')  # í•œêµ­ì–´
#    pipeline = AudioPipeline(target_language='ja')  # ì¼ë³¸ì–´
#
# 4. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬:
#    pipeline = AudioPipeline()
#    result = pipeline.process_single_file("video.mp4")
#
# 5. FFmpeg ì„¤ì¹˜ í™•ì¸:
#    pipeline = AudioPipeline()
#    print(f"FFmpeg ì‚¬ìš© ê°€ëŠ¥: {pipeline.ffmpeg_available}")
#
# ì¶œë ¥ íŒŒì¼:
# - audio_out/: ë…¸ì´ì¦ˆ ì œê±°ëœ ì˜¤ë””ì˜¤
# - script_output/: ì „ì‚¬ í…ìŠ¤íŠ¸ ë° SRT ìë§‰
# - processing_summary_*.txt: ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
"""
audio_pipeline.py íŒŒì¼ ë§¨ ì•„ë˜ì— ì¶”ê°€í•  ì½”ë“œ
(1610ì¤„ ì´í›„ì— ì¶”ê°€)
"""

# ===== ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° ì´ˆê¸°í™” í•¨ìˆ˜ ì¶”ê°€ =====

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
audio_pipeline_instance = None


def initialize_audio_pipeline(
    use_gpu: bool = True,
    target_language: Optional[str] = None,
    whisper_model_size: str = "large-v3",
    load_denoiser: bool = True,
    load_speaker_encoder: bool = True
):
    """
    ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ë° ëª¨ë¸ ë¡œë“œ
    
    Args:
        use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        target_language: ëŒ€ìƒ ì–¸ì–´ (None=ìë™ê°ì§€)
        whisper_model_size: Whisper ëª¨ë¸ í¬ê¸°
        load_denoiser: ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë“œ ì—¬ë¶€
        load_speaker_encoder: í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì—¬ë¶€
    """
    global audio_pipeline_instance
    
    logger.info("="*60)
    logger.info("ğŸš€ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("="*60)
    
    try:
        # GPU í™•ì¸
        if use_gpu and not torch.cuda.is_available():
            logger.warning("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            use_gpu = False
        
        if use_gpu:
            gpu_name = torch.cuda.get_device_name(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"âœ… GPU: {gpu_name} ({total_memory:.1f}GB)")
        else:
            logger.info("âš ï¸  CPU ëª¨ë“œ")
        
        # AudioPipeline ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        logger.info("\n[1/4] AudioPipeline ì¸ìŠ¤í„´ìŠ¤ ìƒì„±...")
        audio_pipeline_instance = AudioPipeline(
            use_gpu=use_gpu,
            target_language=target_language
        )
        logger.info(f"   ë””ë°”ì´ìŠ¤: {audio_pipeline_instance.device}")
        
        # Whisper ëª¨ë¸ ë¡œë“œ
        logger.info(f"\n[2/4] Whisper ëª¨ë¸ ë¡œë”© ({whisper_model_size})...")
        audio_pipeline_instance._load_whisper(model_size=whisper_model_size)
        logger.info("âœ… Whisper ë¡œë”© ì™„ë£Œ")
        
        if use_gpu:
            allocated = torch.cuda.memory_allocated() / 1e9
            logger.info(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©: {allocated:.2f}GB")
        
        # ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë“œ
        if load_denoiser:
            logger.info("\n[3/4] ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”©...")
            try:
                audio_pipeline_instance._load_denoiser()
                logger.info("âœ… ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸  ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            logger.info("\n[3/4] ë…¸ì´ì¦ˆ ì œê±° ëª¨ë¸ ìŠ¤í‚µ")
        
        # í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ
        if load_speaker_encoder:
            logger.info("\n[4/4] í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”©...")
            try:
                success = audio_pipeline_instance._load_speaker_encoder()
                if success:
                    logger.info("âœ… í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸  í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            except Exception as e:
                logger.warning(f"âš ï¸  í™”ìë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            logger.info("\n[4/4] í™”ìë¶„ë¦¬ ëª¨ë¸ ìŠ¤í‚µ")
        
        logger.info("\n" + "="*60)
        logger.info("âœ… ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info("="*60)
        
        return audio_pipeline_instance
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        audio_pipeline_instance = None
        raise


def get_pipeline_status() -> dict:
    """í˜„ì¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ"""
    if audio_pipeline_instance is None:
        return {
            "initialized": False,
            "error": "íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    status = {
        "initialized": True,
        "device": audio_pipeline_instance.device,
        "use_gpu": audio_pipeline_instance.use_gpu,
        "target_language": audio_pipeline_instance.target_language or "auto",
        "models": {
            "whisper": audio_pipeline_instance.whisper_model is not None,
            "denoiser": audio_pipeline_instance.denoiser is not None,
            "speaker_encoder": audio_pipeline_instance.speaker_encoder is not None
        }
    }
    
    if audio_pipeline_instance.use_gpu and torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        status["gpu_memory"] = {
            "allocated_gb": round(allocated, 2),
            "total_gb": round(total, 2),
            "usage_percent": round((allocated / total) * 100, 1)
        }
    
    return status


def get_memory_stats() -> dict:
    """GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    if audio_pipeline_instance is None or not audio_pipeline_instance.use_gpu:
        return {"error": "GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    
    if not torch.cuda.is_available():
        return {"error": "GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    