
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
from pathlib import Path
from typing import Optional, Dict
import re

from .base import BaseTranslator, TranslationResult

logger = logging.getLogger(__name__)


class QwenLocalTranslator(BaseTranslator):
    """qwen 14b ì‚¬ìš© (8bë„ ì§€ì›)"""
    
    # 14B ëª¨ë¸ ìµœì í™” ì„¤ì • (24GB GPU ê¸°ì¤€, 4bit ì–‘ìí™” ì‚¬ìš© ì‹œ ì•½ 8-10GB)
    MAX_INPUT_LENGTH = 4096
    MAX_OUTPUT_CAP = 4096
    MIN_OUTPUT_TOKENS = 512
    
    def __init__(
        self,
        model_path: str,
        use_gpu: bool = True,
        load_in_4bit: bool = True
    ):
        """
        Args:
            model_path: LoRA ëª¨ë¸ ê²½ë¡œ
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            load_in_4bit: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
        """
        super().__init__("qwen-local")
        self.model_path = Path(model_path)
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.load_in_4bit = load_in_4bit
        
        self.model = None
        self.tokenizer = None
        
        logger.info(f"QwenLocalTranslator ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
        if self.use_gpu:
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"GPU ë©”ëª¨ë¦¬: {total_memory:.2f} GB")
    
    def load_model(self, **kwargs):
        """ëª¨ë¸ ë¡œë”©"""
        if self.model is not None:
            logger.warning("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            self._loaded = True
            return
        
        try:
            logger.info(f"ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
            
            # Tokenizer
            logger.info("Tokenizer ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            # 4bit ì–‘ìí™”
            if self.load_in_4bit and self.use_gpu:
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                )
                
                logger.info("4bit ì–‘ìí™” í™œì„±í™”")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.use_gpu else torch.float32
                )
            
            self.model.eval()
            self._loaded = True
            
            logger.info("[OK] ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            logger.info(f"   4bit ì–‘ìí™”: {self.load_in_4bit}")
            
            if self.use_gpu:
                allocated = torch.cuda.memory_allocated() / 1e9
                logger.info(f"   ëª¨ë¸ ë©”ëª¨ë¦¬: {allocated:.2f} GB")
            
        except Exception as e:
            # Windows ì½˜ì†” í˜¸í™˜ì„±ì„ ìœ„í•´ ì´ëª¨ì§€ ì œê±°
            error_msg = f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            print(f"[ERROR] {error_msg}")
            raise
    
    def translate(
        self,
        text: str,
        source_lang: str = "ko",
        target_lang: str = "ja",
        max_new_tokens: Optional[int] = None,
        temperature: float = 0.1,
        top_p: float = 0.9,
        do_sample: bool = True,
        enable_diarization: bool = True,
        **kwargs
    ) -> TranslationResult:
        """í…ìŠ¤íŠ¸ ë²ˆì—­ (ìë™ í˜•ì‹ ê°ì§€)"""
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # í˜•ì‹ ìë™ ê°ì§€
        format_type = self._detect_format(text)
        
        # ìë™ ë¼ìš°íŒ…
        if format_type == "transcript":
            logger.info("[TRANSCRIPT] ìë™ ê°ì§€: íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ í˜•ì‹")
            result_dict = self._translate_transcript(
                text, source_lang, target_lang,
                temperature, top_p, do_sample,
                enable_diarization
            )
        elif format_type == "multiline":
            logger.info("[MULTILINE] ìë™ ê°ì§€: ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸")
            result_dict = self._translate_multiline(
                text, source_lang, target_lang,
                max_new_tokens, temperature, top_p, do_sample
            )
        else:
            logger.info("[SINGLE] ìë™ ê°ì§€: ì¼ë°˜ í…ìŠ¤íŠ¸")
            result_dict = self._translate_single(
                text, source_lang, target_lang,
                max_new_tokens, temperature, top_p, do_sample
            )
        
        # TranslationResultë¡œ ë³€í™˜
        return TranslationResult(
            original_text=result_dict["original_text"],
            translated_text=result_dict["translated_text"],
            source_lang=result_dict["source_lang"],
            target_lang=result_dict["target_lang"],
            input_tokens=result_dict["input_tokens"],
            output_tokens=result_dict["output_tokens"],
            model_name=self.model_name
        )
    
    def _translate_single(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        max_new_tokens: Optional[int],
        temperature: float,
        top_p: float,
        do_sample: bool
    ) -> Dict[str, any]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        try:
            logger.info(f"ë²ˆì—­ ì‹œì‘: {source_lang} â†’ {target_lang}")
            logger.info(f"ì›ë¬¸ ê¸¸ì´: {len(text)} ê¸€ì")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(text, source_lang, target_lang)
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.MAX_INPUT_LENGTH
            ).to(self.device)
            
            input_length = inputs['input_ids'].shape[1]
            logger.info(f"ì…ë ¥ í† í° ê¸¸ì´: {input_length}")
            
            # max_new_tokens ë™ì  ê³„ì‚°
            if max_new_tokens is None:
                calculated_tokens = int(input_length * 1.5) + 200
                max_new_tokens = min(
                    max(calculated_tokens, self.MIN_OUTPUT_TOKENS),
                    self.MAX_OUTPUT_CAP
                )
                logger.info(f"[OK] ë™ì  max_new_tokens: {max_new_tokens}")
            else:
                max_new_tokens = min(max_new_tokens, self.MAX_OUTPUT_CAP)
            
            # ë©”ëª¨ë¦¬ ì²´í¬ ë¡œì§ì€ í•„ìš” ì‹œ ë””ë²„ê¹…ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (ë¡œê·¸ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•´ ê¸°ë³¸ ë¹„í™œì„±í™”)
            # if self.use_gpu:
            #     allocated = torch.cuda.memory_allocated() / 1e9
            #     reserved = torch.cuda.memory_reserved() / 1e9
            #     total = torch.cuda.get_device_properties(0).total_memory / 1e9
            #     free_gb = total - allocated
            #     logger.info(f"VRAM ìƒíƒœ - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB, ì—¬ìœ : {free_gb:.2f}GB / {total:.2f}GB")
            #     
            #     # 14B ëª¨ë¸(4bit)ì€ ì•½ 8-10GB í•„ìš”í•˜ë¯€ë¡œ 3GB ì´í•˜ë©´ ê²½ê³ 
            #     if free_gb < 3.0:
            #         logger.warning("[WARNING] VRAM ë¶€ì¡± ê²½ê³ ! (14B ëª¨ë¸ì€ ìµœì†Œ 8-10GB í•„ìš”)")
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3
                )
            
            output_length = outputs.shape[1]
            actual_generated = output_length - input_length
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
            translated_text = self._extract_translation(generated_text, prompt)
            
            logger.info(f"[OK] ë²ˆì—­ ì™„ë£Œ: {len(translated_text)} ê¸€ì")
            
            if self.use_gpu:
                # 14B ëª¨ë¸ ì‚¬ìš© í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ê°•í™”
                torch.cuda.empty_cache()
                import gc
                gc.collect()
            
            return {
                "original_text": text,
                "translated_text": translated_text,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "input_tokens": input_length,
                "output_tokens": actual_generated
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error("[ERROR] GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!")
                if self.use_gpu:
                    torch.cuda.empty_cache()
                raise RuntimeError("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. í…ìŠ¤íŠ¸ë¥¼ ë” ì§§ê²Œ ë‚˜ëˆ„ì„¸ìš”.") from e
            raise
        except Exception as e:
            logger.error(f"[ERROR] ë²ˆì—­ ì‹¤íŒ¨: {e}")
            raise
    
    def _translate_multiline(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        max_new_tokens: Optional[int],
        temperature: float,
        top_p: float,
        do_sample: bool
    ) -> Dict[str, any]:
        """ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        lines = text.strip().split('\n')
        translated_lines = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        logger.info(f"ì¤„ ë‹¨ìœ„ ë²ˆì—­: {len(lines)}ì¤„")
        
        for i, line in enumerate(lines, 1):
            if line.strip():
                try:
                    result = self._translate_single(
                        text=line.strip(),
                        source_lang=source_lang,
                        target_lang=target_lang,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=do_sample
                    )
                    translated_lines.append(result['translated_text'])
                    total_input_tokens += result['input_tokens']
                    total_output_tokens += result['output_tokens']
                    logger.debug(f"  {i}/{len(lines)} ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"  {i}ë²ˆì§¸ ì¤„ ì‹¤íŒ¨: {e}")
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨: {line}]")
            else:
                translated_lines.append('')
        
        logger.info(f"[OK] ì¤„ ë‹¨ìœ„ ë²ˆì—­ ì™„ë£Œ: {len(lines)}ì¤„")
        
        return {
            "original_text": text,
            "translated_text": '\n'.join(translated_lines),
            "source_lang": source_lang,
            "target_lang": target_lang,
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens
        }
    
    def _translate_transcript(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        temperature: float,
        top_p: float,
        do_sample: bool,
        enable_diarization: bool = True
    ) -> Dict[str, any]:
        """íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­"""
        lines = text.strip().split('\n')
        translated_lines = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        logger.info(f"íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­: {len(lines)}ì¤„ (í™”ìë¶„ë¦¬: {'ON' if enable_diarization else 'OFF'})")
        print(f"\n[DEBUG] ===== _translate_transcript ì‹œì‘ =====")
        print(f"[DEBUG] enable_diarization = {enable_diarization} (type: {type(enable_diarization)})")
        print(f"[DEBUG] í™”ìë¶„ë¦¬ ëª¨ë“œ: {'ON' if enable_diarization else 'OFF'}")
        logger.info(f"ì›ë³¸ í…ìŠ¤íŠ¸ ìƒ˜í”Œ (ì²« 3ì¤„):")
        for i, line in enumerate(lines[:3], 1):
            logger.info(f"  {i}: {line[:100]}")
        
        # ì™„ì „íˆ ë¶„ë¦¬ëœ ì²˜ë¦¬: Trueì™€ FalseëŠ” ë…ë¦½ì ì¸ ë¡œì§
        if enable_diarization:
            print(f"[DEBUG] ===== í™”ìë¶„ë¦¬ ON ë¸”ë¡ ì‹¤í–‰ =====\n")
            # ===== í™”ìë¶„ë¦¬ ON: [íƒ€ì„ìŠ¤íƒ¬í”„] í™”ì: ë‚´ìš© í˜•ì‹ =====
            pattern_with_speaker = r'^(\[[^\]]+\])?\s*(í™”ì\d+|Speaker\d+|[^:]+):\s*(.+)$'
            
            for i, line in enumerate(lines, 1):
                line = line.strip()
                
                if not line:
                    translated_lines.append('')
                    continue
                
                try:
                    match = re.match(pattern_with_speaker, line)
                    
                    if match:
                        timestamp = match.group(1) or ''
                        speaker = match.group(2)
                        content = match.group(3)
                        
                        # ë‚´ìš©ë§Œ ë²ˆì—­
                        text_to_translate = content.strip()
                        
                        # ë²ˆì—­
                        result = self._translate_single(
                            text=text_to_translate,
                            source_lang=source_lang,
                            target_lang=target_lang,
                            max_new_tokens=None,
                            temperature=temperature,
                            top_p=top_p,
                            do_sample=do_sample
                        )
                        
                        # ë²ˆì—­ ê²°ê³¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì œê±°
                        translated_text = self._remove_timestamps_from_text(result['translated_text'])
                        
                        # ì¬ì¡°ë¦½: íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í™”ì ëª¨ë‘ ë‹¤ì‹œ ë¶™ì„
                        if timestamp:
                            reconstructed = f"{timestamp} {speaker}: {translated_text}"
                        else:
                            reconstructed = f"{speaker}: {translated_text}"
                        
                        translated_lines.append(reconstructed)
                        total_input_tokens += result['input_tokens']
                        total_output_tokens += result['output_tokens']
                        logger.debug(f"  {i}/{len(lines)} [{speaker}] ì™„ë£Œ")
                    else:
                        # íŒ¨í„´ ë¶ˆì¼ì¹˜ - ì¼ë°˜ ë²ˆì—­
                        cleaned_line = self._remove_timestamps_from_text(line)
                        if not cleaned_line:
                            translated_lines.append('')
                            continue
                        
                        result = self._translate_single(
                            text=cleaned_line,
                            source_lang=source_lang,
                            target_lang=target_lang,
                            max_new_tokens=None,
                            temperature=temperature,
                            top_p=top_p,
                            do_sample=do_sample
                        )
                        translated_text = self._remove_timestamps_from_text(result['translated_text'])
                        translated_lines.append(translated_text)
                        total_input_tokens += result['input_tokens']
                        total_output_tokens += result['output_tokens']
                        
                except Exception as e:
                    logger.error(f"  {i}ë²ˆì§¸ ì¤„ ì‹¤íŒ¨: {e}")
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨] {line}")
        else:
            print(f"[DEBUG] ===== í™”ìë¶„ë¦¬ OFF ë¸”ë¡ ì‹¤í–‰ =====\n")
            # ===== í™”ìë¶„ë¦¬ OFF: [íƒ€ì„ìŠ¤íƒ¬í”„] ë‚´ìš© í˜•ì‹ (í™”ì ì •ë³´ ì—†ìŒ) =====
            # Whisper ì •í™•í•œ í˜•ì‹ë§Œ ë§¤ì¹­: [MM:SS.mmm] ë˜ëŠ” [HH:MM:SS.mmm]
            # Optional(?) ì œê±°: íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨
            # ^\s* ì¶”ê°€: ì¤„ ì‹œì‘ì˜ ê³µë°±/BOM í—ˆìš©
            pattern_without_speaker = r'^\s*(\[\d{2}:\d{2}(?::\d{2})?\.\d{3}\])\s+(.*)$'
            
            print(f"[DEBUG] í™”ìë¶„ë¦¬ OFF ëª¨ë“œë¡œ ì²˜ë¦¬ ì‹œì‘ (íŒ¨í„´: {pattern_without_speaker})")
            
            for i, line in enumerate(lines, 1):
                # ì›ë³¸ ë¼ì¸ ë³´ì¡´ (ë””ë²„ê¹…ìš©)
                original_line = line
                # BOM ì œê±°ëŠ” íŒ¨í„´ì—ì„œ \s*ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” stripë§Œ
                line = line.strip()
                
                if not line:
                    translated_lines.append('')
                    continue
                
                try:
                    print(f"[DEBUG] ì¤„ {i}: ì›ë³¸ = '{original_line[:80]}...'")
                    print(f"[DEBUG] ì¤„ {i}: ì •ë¦¬ í›„ = '{line[:80]}...'")
                    match = re.match(pattern_without_speaker, line)
                    print(f"[DEBUG] ì¤„ {i}: íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼ = {match is not None}")
                    
                    if match:
                        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì •í™•íˆ ë§¤ì¹­ë¨
                        timestamp = match.group(1)
                        content = match.group(2)
                        
                        # ë””ë²„ê¹…: ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ í™•ì¸ (printë¡œë„ ì¶œë ¥í•˜ì—¬ í™•ì‹¤íˆ í™•ì¸)
                        print(f"\n[DEBUG] ì¤„ {i}: ì›ë³¸ ë¼ì¸ = {line}")
                        print(f"[DEBUG] ì¤„ {i}: ì¶”ì¶œëœ íƒ€ì„ìŠ¤íƒ¬í”„ = '{timestamp}'")
                        print(f"[DEBUG] ì¤„ {i}: ì¶”ì¶œëœ ë‚´ìš© = '{content[:50]}...'")
                        logger.info(f"  ì¤„ {i}: ì›ë³¸ ë¼ì¸ = {line}")
                        logger.info(f"  ì¤„ {i}: ì¶”ì¶œëœ íƒ€ì„ìŠ¤íƒ¬í”„ = '{timestamp}'")
                        logger.info(f"  ì¤„ {i}: ì¶”ì¶œëœ ë‚´ìš© = '{content[:50]}...'")
                        
                        # ë‚´ìš©ë§Œ ë²ˆì—­ (íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì œì™¸)
                        text_to_translate = content.strip()
                        
                        # ë””ë²„ê¹…: LLMì— ì „ë‹¬ë˜ëŠ” í…ìŠ¤íŠ¸ í™•ì¸ (íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸)
                        print(f"[DEBUG] ì¤„ {i}: LLMì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ = '{text_to_translate[:80]}...'")
                        logger.info(f"  ì¤„ {i}: LLMì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ = '{text_to_translate[:80]}...'")
                        if '[' in text_to_translate and ']' in text_to_translate:
                            print(f"[WARNING] ì¤„ {i}: LLMì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ì— íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                            logger.warning(f"  âš ï¸ ì¤„ {i}: LLMì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ì— íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
                        
                        # ë²ˆì—­
                        result = self._translate_single(
                            text=text_to_translate,
                            source_lang=source_lang,
                            target_lang=target_lang,
                            max_new_tokens=None,
                            temperature=temperature,
                            top_p=top_p,
                            do_sample=do_sample
                        )
                        
                        # ë²ˆì—­ ê²°ê³¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì œê±° (ê°•í™”)
                        translated_text = self._remove_timestamps_from_text(result['translated_text'])
                        
                        # ì¬ì¡°ë¦½: ì›ë³¸ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³€í˜•í•˜ì§€ ì•ŠìŒ)
                        reconstructed = f"{timestamp} {translated_text}"
                        logger.info(f"  ì¤„ {i}: ì¬ì¡°ë¦½ ê²°ê³¼ = '{reconstructed[:80]}...'")
                        
                        translated_lines.append(reconstructed)
                        total_input_tokens += result['input_tokens']
                        total_output_tokens += result['output_tokens']
                        logger.debug(f"  {i}/{len(lines)} ì™„ë£Œ")
                    else:
                        # íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ - íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ëŠ” ì¤„ì´ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ë¨
                        logger.warning(f"  ì¤„ {i}: íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ - '{line[:50]}...'")
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ëŠ” ì¤„ë¡œ ê°„ì£¼í•˜ê³  ì „ì²´ë¥¼ ë²ˆì—­
                        # í•˜ì§€ë§Œ ë¨¼ì € íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ì´ ìˆëŠ”ì§€ í™•ì¸
                        if re.search(r'\[\d{1,2}[:ï¼š]\d{1,2}', line):
                            # íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ì´ ìˆì§€ë§Œ ì •í™•íˆ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ
                            # íƒ€ì„ìŠ¤íƒ¬í”„ ë¶€ë¶„ì„ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ë§Œ ë²ˆì—­
                            cleaned_line = self._remove_timestamps_from_text(line)
                            if cleaned_line and cleaned_line.strip():
                                text_to_translate = cleaned_line.strip()
                            else:
                                # íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ìˆëŠ” ì¤„
                                translated_lines.append(line)
                                continue
                        else:
                            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸
                            text_to_translate = line
                        
                        # ë²ˆì—­
                        result = self._translate_single(
                            text=text_to_translate,
                            source_lang=source_lang,
                            target_lang=target_lang,
                            max_new_tokens=None,
                            temperature=temperature,
                            top_p=top_p,
                            do_sample=do_sample
                        )
                        
                        # ë²ˆì—­ ê²°ê³¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì œê±°
                        translated_text = self._remove_timestamps_from_text(result['translated_text'])
                        translated_lines.append(translated_text)
                        total_input_tokens += result['input_tokens']
                        total_output_tokens += result['output_tokens']
                        
                except Exception as e:
                    logger.error(f"  {i}ë²ˆì§¸ ì¤„ ì‹¤íŒ¨: {e}")
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨] {line}")
        
        logger.info(f"[OK] íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­ ì™„ë£Œ: {len(lines)}ì¤„")
        
        return {
            "original_text": text,
            "translated_text": '\n'.join(translated_lines),
            "source_lang": source_lang,
            "target_lang": target_lang,
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens
        }
    
    def _create_prompt(self, text: str, source_lang: str, target_lang: str) -> str:
        """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        lang_map = {
            "ko": "Korean",
            "ja": "Japanese", 
            "en": "English"
        }
        
        source_full = lang_map.get(source_lang.lower(), source_lang)
        target_full = lang_map.get(target_lang.lower(), target_lang)
        direction = f"[{source_full} to {target_full}]"
        
        # ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_content = (
            "You are a professional Korean-Japanese bilingual translator. "
            "Translate ONLY the given text accurately without any explanations, "
            "notes, or additional content. Do not mix other languages. "
            "Output only the translated text."
        )
        
        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user", 
                "content": f"{direction}\n{text}"
            }
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        
        return prompt

    def _remove_timestamps_from_text(self, text: str) -> str:
        """ë²ˆì—­ ê²°ê³¼ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì œê±° (ê¸°ê³„ì  ì²˜ë¦¬, LLM ìƒì„± íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)"""
        if not text:
            return text
        
        result = text
        
        # 1. ì •í™•í•œ Whisper í˜•ì‹ ì œê±°
        exact_patterns = [
            r'\[\d{2}:\d{2}\.\d{3}\]',  # [00:35.560] í˜•ì‹
            r'\[\d{2}:\d{2}:\d{2}\.\d{3}\]',  # [00:04:08.000] í˜•ì‹
        ]
        
        for pattern in exact_patterns:
            result = re.sub(pattern, '', result)
        
        # 2. LLMì´ ìƒì„±í•œ ë³€í˜•ëœ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±° (ë¶ˆì™„ì „í•œ í˜•ì‹ í¬í•¨)
        weird_patterns = [
            r'\[\d{1,2}:\s*\d{1,2}:\d+\]',  # [00: 00:001] í˜•ì‹
            r'\[\d{1,2}:\s*\d{1,2}:\d+:\d+\]',  # [00: 21:18:0] í˜•ì‹
            r'\[\d{1,2}:\s*\d+,\d+ì´ˆ?\]',  # [00: 19,62ì´ˆ] í˜•ì‹
            r'\[\d{1,2}:\s*\d+:\d+\]',  # [00: 32:82] í˜•ì‹
            r'\[\d{1,2}:\d{2}(?:\.\d+)?\]',  # [00:35.560] ë˜ëŠ” [00:35] í˜•ì‹
            r'\[\d{1,2}:\d{2}:\d{2}(?:\.\d+)?\]',  # [00:04:08] í˜•ì‹
        ]
        
        for pattern in weird_patterns:
            result = re.sub(pattern, '', result)
        
        # 3. ë¶ˆì™„ì „í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±° (ë‹«ëŠ” ê´„í˜¸ê°€ ì—†ëŠ” ê²½ìš°)
        incomplete_patterns = [
            r'\[\d{1,2}:\s*\d{1,2}:\d+[^\]]*$',  # [00: 00:001 (ì¤„ ë)
            r'\[\d{1,2}:\s*\d+[^\]]*$',  # [00: 35 (ì¤„ ë)
            r'\[\d{1,2}:\d{2}[^\]]*$',  # [00:35 (ì¤„ ë)
        ]
        
        for pattern in incomplete_patterns:
            result = re.sub(pattern, '', result, flags=re.MULTILINE)
        
        # 4. ì¤‘ì²©ëœ ëŒ€ê´„í˜¸ë‚˜ ì´ìƒí•œ í˜•ì‹ ì œê±°
        result = re.sub(r'\[[^\]]*\[', '', result)  # [xxx[ í˜•ì‹
        result = re.sub(r'\[[^\]]*$', '', result, flags=re.MULTILINE)  # ì¤„ ëì˜ ë¶ˆì™„ì „í•œ ëŒ€ê´„í˜¸
        
        # 5. ê¸°íƒ€ ëª¨ë“  [xxx] í˜•ì‹ ì œê±° (ë§ˆì§€ë§‰ì— ì‹¤í–‰)
        result = re.sub(r'\[[^\]]+\]', '', result)
        
        # 6. ì—°ì†ëœ ê³µë°± ì •ë¦¬
        result = re.sub(r'\s+', ' ', result).strip()
        
        return result
    
    def _extract_translation(self, generated_text: str, prompt: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶”ì¶œ"""
        result = generated_text
        
        # 1. í”„ë¡¬í”„íŠ¸ ì œê±°
        if prompt in result:
            result = result.replace(prompt, "").strip()
        
        # 2. special tokens ì œê±°
        special_tokens = [
            "<|im_start|>", "<|im_end|>", "<|endoftext|>",
            "system\n", "user\n", "assistant\n",
            "<|system|>", "<|user|>", "<|assistant|>"
        ]
        
        for token in special_tokens:
            result = result.replace(token, "")
        
        # 3. íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì œê±° (ë²ˆì—­ ê²°ê³¼ì— í¬í•¨ë  ìˆ˜ ìˆìŒ)
        result = self._remove_timestamps_from_text(result)
        
        # 4. ì„¤ëª… íŒ¨í„´ ì œê±°
        explanation_patterns = [
            r'[.ã€‚]?\s*ì´\s*ë¬¸ì¥[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ë¼ëŠ”\s*ì˜ë¯¸[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ë¥¼\s*ëœ»[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ì›ë¬¸ì˜\s*ë§¥ë½[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ìì—°ìŠ¤ëŸ½ê²Œ\s*í‘œí˜„í•˜ë©´[^.ã€‚]*[.ã€‚]',
            r'\*\*[^*]+\*\*',
            r'ë˜ëŠ”\s*\n',
        ]
        
        for pattern in explanation_patterns:
            result = re.sub(pattern, '', result, flags=re.IGNORECASE)
        
        # 5. ë‹¤ë¥¸ ì–¸ì–´ ì„ì„ ê°ì§€
        english_ratio = len(re.findall(r'[a-zA-Z]{3,}', result)) / max(len(result.split()), 1)
        if english_ratio > 0.3:
            logger.warning(f"ì˜ì–´ ë¹„ìœ¨ ë†’ìŒ: {english_ratio:.2%}")
        
        # 6. ì¤„ ë‹¨ìœ„ë¡œ ë¼ë²¨ ì œê±°
        lines = result.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line.lower() in ['system', 'user', 'assistant']:
                continue
            if len(line) > 0 and not line.replace(' ', '').replace('*', '').replace('-', ''):
                continue
            if line:
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines)
        result = result.strip()
        
        # 7. ìµœì¢… ê²€ì¦
        if not result:
            logger.warning("[WARNING] ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            logger.debug(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text[:200]}...")
    
        return result
    
    def get_memory_stats(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if not self.use_gpu:
            return {"message": "GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
        
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        free = total - allocated
        
        return {
            "total_gb": round(total, 2),
            "allocated_gb": round(allocated, 2),
            "reserved_gb": round(reserved, 2),
            "free_gb": round(free, 2)
        }
    
    def unload_model(self):
        """ë©”ëª¨ë¦¬ í•´ì œ (14B ëª¨ë¸ ìµœì í™”)"""
        if self.model is not None:
            try:
                logger.info("ğŸ”„ Qwen ëª¨ë¸ GPU ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...")
                
                # GPUì—ì„œ CPUë¡œ ì´ë™ (GPU ë©”ëª¨ë¦¬ í™•ë³´)
                if hasattr(self.model, 'to'):
                    try:
                        self.model.to('cpu')
                    except Exception as e:
                        logger.debug(f"ëª¨ë¸ CPU ì´ë™ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ CPUë¡œ ëª…ì‹œì ìœ¼ë¡œ ì´ë™
                if hasattr(self.model, 'parameters'):
                    for param in self.model.parameters():
                        if param.is_cuda:
                            try:
                                param.data = param.data.cpu()
                            except Exception:
                                pass
                
                # ëª¨ë¸ì˜ ëª¨ë“  ë²„í¼ë¥¼ CPUë¡œ ì´ë™
                if hasattr(self.model, 'buffers'):
                    for buffer in self.model.buffers():
                        if buffer.is_cuda:
                            try:
                                buffer.data = buffer.data.cpu()
                            except Exception:
                                pass
                
                # 4bit ì–‘ìí™” ëª¨ë¸ì˜ íŠ¹ë³„ ì²˜ë¦¬
                if self.load_in_4bit:
                    try:
                        # BitsAndBytesConfigë¡œ ë¡œë“œëœ ëª¨ë¸ì˜ íŠ¹ë³„ ì²˜ë¦¬
                        if hasattr(self.model, 'model'):
                            # PEFTë‚˜ ì–‘ìí™” ë˜í¼ ì œê±°
                            inner_model = getattr(self.model, 'model', None)
                            if inner_model is not None:
                                if hasattr(inner_model, 'to'):
                                    try:
                                        inner_model.to('cpu')
                                    except Exception:
                                        pass
                    except Exception as e:
                        logger.debug(f"4bit ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                
                # ëª¨ë¸ ì‚­ì œ
                del self.model
                self.model = None
                
                # Tokenizerë„ ì •ë¦¬
                if self.tokenizer is not None:
                    del self.tokenizer
                    self.tokenizer = None
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰ (ë‘ ë²ˆ ì‹¤í–‰í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ì •ë¦¬)
                import gc
                gc.collect()
                gc.collect()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if self.use_gpu and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    try:
                        torch.cuda.reset_peak_memory_stats()
                    except Exception:
                        pass
                    
                    # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
                    allocated = torch.cuda.memory_allocated() / 1e9
                    reserved = torch.cuda.memory_reserved() / 1e9
                    logger.info(f"âœ… Qwen ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ (GPU í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB)")
                else:
                    logger.info("âœ… Qwen ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
                
                self._loaded = False
                
            except Exception as e:
                logger.warning(f"Qwen ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                # ì˜¤ë¥˜ê°€ ë‚˜ë„ ìƒíƒœëŠ” ì´ˆê¸°í™”
                self.model = None
                self.tokenizer = None
                self._loaded = False
                if self.use_gpu and torch.cuda.is_available():
                    torch.cuda.empty_cache()

                    

