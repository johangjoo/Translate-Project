"""
API ë¼ìš°íŠ¸ - ê°„ì†Œí™” ë²„ì „
- ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ (ë…¸ì´ì¦ˆ ì œê±° + STT + í™”ìë¶„ë¦¬)
- í…ìŠ¤íŠ¸ ë²ˆì—­
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from pydantic import BaseModel, Field
from pathlib import Path
import os
import uuid
import shutil
import time
import re
from typing import Optional, List, Dict

# models.pyì—ì„œ import
from .models import (
    TranslationResponse,
    AudioProcessResponse,
    AudioHealthResponse
)

router = APIRouter()

# ë²ˆì—­/ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  í´ë˜ìŠ¤ë“¤ (ìš”ì²­ ì‹œ ë¡œë”©)
from api.translation import create_translator, TranslationModelType
from api.audio_pipeline import AudioPipeline

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)


# ===== Pydantic ëª¨ë¸ (routes ì „ìš©) =====

class BasicHealthResponse(BaseModel):
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str
    stt_loaded: bool
    translator_loaded: bool
    stt_device: str
    translator_device: str


# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====

def save_upload_file(upload_file: UploadFile, max_size_mb: int = 200) -> str:
    """ì—…ë¡œë“œ íŒŒì¼ ì €ì¥"""
    upload_file.file.seek(0, 2)
    file_size = upload_file.file.tell()
    upload_file.file.seek(0)
    
    max_size = max_size_mb * 1024 * 1024
    if file_size > max_size:
        raise HTTPException(
            status_code=413,
            detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€: {max_size_mb}MB"
        )
    
    file_ext = os.path.splitext(upload_file.filename)[1]
    if not file_ext:
        file_ext = ".wav"
    
    temp_filename = f"{uuid.uuid4()}{file_ext}"
    temp_path = UPLOAD_DIR / temp_filename
    
    with open(temp_path, "wb") as buffer:
        shutil.copyfileobj(upload_file.file, buffer)
    
    return str(temp_path)


def cleanup_file(file_path: str):
    """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
    except Exception as e:
        print(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")


def parse_transcript_segments(transcript_path: str) -> List:
    """ì „ì‚¬ íŒŒì¼ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ"""
    from .models import AudioSegment
    
    segments = []
    
    try:
        if not os.path.exists(transcript_path):
            return segments
        
        with open(transcript_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # í˜•ì‹: [mm:ss - mm:ss] í™”ìX: í…ìŠ¤íŠ¸
            pattern = r'\[(\d+):(\d+\.\d+)\s*-\s*(\d+):(\d+\.\d+)\]\s*(Speaker\d+|í™”ì\d+):\s*(.+)'
            match = re.match(pattern, line)
            
            if match:
                start_min, start_sec, end_min, end_sec, speaker, text = match.groups()
                start = int(start_min) * 60 + float(start_sec)
                end = int(end_min) * 60 + float(end_sec)
                
                segments.append(AudioSegment(
                    start=start,
                    end=end,
                    text=text.strip(),
                    speaker=speaker
                ))
    
    except Exception as e:
        print(f"ì„¸ê·¸ë¨¼íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    return segments


# ===== 1. ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ =====

@router.post("/audio/process", response_model=AudioProcessResponse)
async def process_audio(
    audio_file: UploadFile = File(..., description="ìŒì„± íŒŒì¼"),
    enable_denoise: bool = Form(True, description="ë…¸ì´ì¦ˆ ì œê±° í™œì„±í™”"),
    enable_transcription: bool = Form(True, description="STT í™œì„±í™”"),
    enable_diarization: bool = Form(True, description="í™”ìë¶„ë¦¬ í™œì„±í™”"),
    language: Optional[str] = Form(None, description="ì–¸ì–´ ì½”ë“œ (None=ìë™ê°ì§€)"),
    create_srt: bool = Form(True, description="SRT ìë§‰ íŒŒì¼ ìƒì„±"),
    save_outputs: bool = Form(True, description="ê²°ê³¼ íŒŒì¼ ì €ì¥"),
    max_speakers: int = Form(2, description="ìµœëŒ€ í™”ì ìˆ˜ (1~10)")
):
   
   
    pipeline: AudioPipeline = AudioPipeline(
        use_gpu=True,
        target_language=language or None,
    )

    # ìµœëŒ€ í™”ì ìˆ˜ ì„¤ì • (1~10 ë²”ìœ„ë¡œ í´ë¨í”„)
    try:
        if max_speakers is not None:
            clamped = max(1, min(10, int(max_speakers)))
            pipeline.max_speakers = clamped
    except Exception:
        # ì˜ëª»ëœ ê°’ì´ ë“¤ì–´ì™€ë„ ê¸°ë³¸ê°’(2)ì„ ìœ ì§€
        pass
    
    temp_path = None
    total_start = time.time()
    timing = {}
    
    result = {
        "original_filename": audio_file.filename,
        "denoised": enable_denoise,
        "transcribed": enable_transcription,
        "diarization_enabled": enable_diarization,
    }
    
    try:
        temp_path = save_upload_file(audio_file)
        print(f"\n{'='*60}")
        print(f"íŒŒì¼ ì—…ë¡œë“œ: {audio_file.filename}")
        print(f"{'='*60}\n")
        
        work_dir = UPLOAD_DIR / f"work_{uuid.uuid4().hex[:8]}"
        work_dir.mkdir(exist_ok=True)
        
        current_file = temp_path
        
        # 1. ë…¸ì´ì¦ˆ ì œê±°
        if enable_denoise:
            denoise_start = time.time()
            
            denoised_file = work_dir / f"{Path(audio_file.filename).stem}_denoised.wav"
            
            pipeline.denoise_audio(
                input_file=current_file,
                output_file=str(denoised_file)
            )
            
            timing["denoise"] = time.time() - denoise_start
            result["denoised_filename"] = denoised_file.name
            result["denoise_time"] = round(timing["denoise"], 2)
            
            
            current_file = str(denoised_file)
        else:
            result["denoised_filename"] = None
            result["denoise_time"] = None
        
        # 2. STT + í™”ìë¶„ë¦¬
        # 2. STT + í™”ìë¶„ë¦¬ ì„¹ì…˜ì—ì„œ ìˆ˜ì • (224ì¤„ ê·¼ì²˜)

        if enable_transcription:
            transcription_start = time.time()
            
            transcript_result = pipeline.transcribe_uploaded_wav(
                wav_path=current_file,
                save_dir=str(work_dir) if save_outputs else None,
                create_srt=create_srt
            )
            
            timing["transcription"] = time.time() - transcription_start
            
            # âœ… ìˆ˜ì •: simple íŒŒì¼ ë‚´ìš© ì‚¬ìš©
            simple_path = transcript_result.get("simple_path")
            if simple_path and os.path.exists(simple_path):
                with open(simple_path, 'r', encoding='utf-8') as f:
                    result["text"] = f.read()
            else:
                result["text"] = transcript_result["text"]  # fallback
            
            result["detected_language"] = language or "auto"
            result["transcription_time"] = round(timing["transcription"], 2)
            
        else:
            result["text"] = None
            result["detected_language"] = None
            result["transcription_time"] = None
            result["num_speakers"] = None
            result["segments"] = None
        
        total_time = time.time() - total_start
        timing["total"] = total_time
        
        result["processing_time"] = round(total_time, 2)
        result["timing"] = {k: round(v, 2) for k, v in timing.items()}
        
        
        return AudioProcessResponse(**result)

    except Exception as e:
        print(f"\nì˜¤ë¥˜: {str(e)}\n")
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        if temp_path:
            cleanup_file(temp_path)
        # ì‚¬ìš©ì´ ëë‚œ í›„ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•˜ì—¬ VRAMì„ í™•ë³´
        try:
            pipeline.unload_models()
        except Exception:
            pass


# ===== 2. í…ìŠ¤íŠ¸ ë²ˆì—­ =====

@router.post("/translate-text", response_model=TranslationResponse)
async def translate_text_only(
    text: str = Form(..., description="ë²ˆì—­í•  í…ìŠ¤íŠ¸"),
    source_lang: str = Form("ko", description="ì›ë³¸ ì–¸ì–´ (ko, ja, en)"),
    target_lang: str = Form("ja", description="ëª©í‘œ ì–¸ì–´ (ko, ja, en)"),
    model_type: str = Form("qwen-local", description="ë²ˆì—­ ëª¨ë¸ íƒ€ì… (qwen-local, openai, gemini)"),
    api_key: Optional[str] = Form(None, description="API í‚¤ (openai/gemini ì‚¬ìš© ì‹œ í•„ìˆ˜)")
):
   
    start_time = time.time()
    
    try:
        print(f"ğŸŒ í…ìŠ¤íŠ¸ ë²ˆì—­: {source_lang} â†’ {target_lang} (ëª¨ë¸: {model_type})")
        print(f"   ì›ë¬¸: {text[:100]}...")

        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ë²ˆì—­ê¸° ìƒì„±
        translator = None
        
        if model_type == "qwen-local":
            # ë¡œì»¬ Qwen ëª¨ë¸ ê²½ë¡œ ì°¾ê¸° (config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            from api.config import TRANSLATION_BASE_MODEL
            from pathlib import Path as _Path
            
            # config.pyì˜ ê²½ë¡œ ì‚¬ìš©
            model_path = None
            project_root = _Path(__file__).resolve().parent.parent
            
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ ì‹œë„
            possible_paths = [
                Path(TRANSLATION_BASE_MODEL) / "qwen3-8b-lora-10ratio",
                Path(TRANSLATION_BASE_MODEL),
                project_root / "qwen3-8b-lora-10ratio" / "qwen3-8b-lora-10ratio",
                project_root / "qwen3-8b-lora-10ratio",
            ]
            
            # ê²½ë¡œ ì°¾ê¸°
            for path in possible_paths:
                path_obj = Path(path)
                if path_obj.exists() and path_obj.is_dir():
                    # config.jsonì´ë‚˜ tokenizer.jsonì´ ìˆëŠ”ì§€ í™•ì¸
                    if (path_obj / "config.json").exists() or (path_obj / "tokenizer.json").exists():
                        model_path = path_obj
                        print(f"[OK] ëª¨ë¸ ê²½ë¡œ ì°¾ìŒ: {model_path}")
                        break
            
            # ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            if model_path is None:
                error_msg = (
                    f"Qwen ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ì‹œë„í•œ ê²½ë¡œ:\n"
                )
                for path in possible_paths:
                    error_msg += f"  - {path}\n"
                error_msg += f"\napi/config.pyì˜ TRANSLATION_BASE_MODELì„ í™•ì¸í•˜ì„¸ìš”."
                raise HTTPException(status_code=500, detail=error_msg)
            
            translator = create_translator(
                model_type=TranslationModelType.QWEN_LOCAL,
                model_path=str(model_path),
                use_gpu=True,
                load_in_4bit=True
            )
            
        elif model_type == "openai":
            if not api_key:
                raise HTTPException(
                    status_code=400,
                    detail="OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œ api_keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            # ê³ ì • ëª¨ë¸: GPT-5.1
            translator = create_translator(
                model_type=TranslationModelType.OPENAI,
                api_key=api_key,
                model_name="gpt-5.1"
            )
            
        elif model_type == "gemini":
            if not api_key:
                raise HTTPException(
                    status_code=400,
                    detail="Gemini ëª¨ë¸ ì‚¬ìš© ì‹œ api_keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
            
            # ê³ ì • ëª¨ë¸: Gemini 3 Pro Preview (ë¬´ë£Œ í‹°ì–´ì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€)
            # ë¬´ë£Œ í‹°ì–´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ "gemini-1.5-flash"ë¡œ ë³€ê²½í•˜ì„¸ìš”
            translator = create_translator(
                model_type=TranslationModelType.GEMINI,
                api_key=api_key,
                model_name="gemini-2.5-flash"  # ë¬´ë£Œ í‹°ì–´ ë¯¸ì§€ì›, ìœ ë£Œ í”Œëœ í•„ìš”
            )
            
        else:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}. ì§€ì› íƒ€ì…: qwen-local, openai, gemini"
            )
        
        # ëª¨ë¸ ë¡œë“œ ë° ë²ˆì—­ ì‹¤í–‰
        translator.load_model()
        try:
            result = translator.translate(
                text=text,
                source_lang=source_lang,
                target_lang=target_lang,
            )
        finally:
            # ë²ˆì—­ì´ ëë‚˜ë©´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•´ì„œ VRAMì„ ìµœëŒ€í•œ ë¹„ì›Œì¤€ë‹¤
            translator.unload_model()
        
        processing_time = time.time() - start_time
        print(f"âœ… ë²ˆì—­ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
        
        return TranslationResponse(
            original_text=result.original_text,
            translated_text=result.translated_text,
            source_lang=result.source_lang,
            target_lang=result.target_lang,
            audio_filename="N/A",
            processing_time=round(processing_time, 2)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ===== í—¬ìŠ¤ ì²´í¬ =====

@router.get("/health", response_model=BasicHealthResponse)
async def health_check():
    """ê¸°ë³¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    from api import inference
    
    stt_ok = False
    stt_dev = "unknown"
    if hasattr(inference, 'whisper_stt') and inference.whisper_stt is not None:
        if hasattr(inference.whisper_stt, 'model') and inference.whisper_stt.model is not None:
            stt_ok = True
            stt_dev = getattr(inference.whisper_stt, 'device', 'unknown')
    
    # ë²ˆì—­ ëª¨ë¸ì€ ìš”ì²­ ì‹œ ë¡œë“œë˜ë¯€ë¡œ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœë¡œ í‘œì‹œ
    trans_ok = True  # ëª¨ë“ˆí™”ëœ ë²ˆì—­ ì‹œìŠ¤í…œì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
    trans_dev = "on-demand"  # ìš”ì²­ ì‹œ ë¡œë“œ
    
    return BasicHealthResponse(
        status="healthy",
        stt_loaded=stt_ok,
        translator_loaded=trans_ok,
        stt_device=stt_dev,
        translator_device=trans_dev
    )


@router.get("/audio/health", response_model=AudioHealthResponse)
async def audio_health():
    """ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸"""
    from api import audio_pipeline
    
    status = audio_pipeline.get_pipeline_status()
    
    return AudioHealthResponse(
        status="healthy" if status.get("initialized") else "not_initialized",
        initialized=status.get("initialized", False),
        device=status.get("device", "unknown"),
        models=status.get("models", {}),
        gpu_memory=status.get("gpu_memory")
    )


@router.get("/audio/memory")
async def memory_stats():
    """GPU ë©”ëª¨ë¦¬ ìƒíƒœ"""
    from api import audio_pipeline
    return audio_pipeline.get_memory_stats()


@router.get("/languages")
async def get_supported_languages():
    """ì§€ì› ì–¸ì–´ ëª©ë¡"""
    return {
        "stt": {
            "provider": "Whisper",
            "languages": "99ê°œ ì–¸ì–´ ì§€ì›"
        },
        "translation": {
            "provider": "Qwen3-8b LoRA",
            "languages": {
                "ko": "í•œêµ­ì–´",
                "ja": "æ—¥æœ¬èª",
                "en": "English"
            }
        }
    }