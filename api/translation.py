"""
Qwen3-8b LoRA ë²ˆì—­ ëª¨ë“ˆ (í†µí•© ë²„ì „)
ìœ„ì¹˜: api/translation.py

ìë™ í˜•ì‹ ê°ì§€:
- íŠ¸ëœìŠ¤í¬ë¦½íŠ¸: [mm:ss] í™”ì: ë‚´ìš©
- ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸: ê°€ì‚¬, ì‹œ ë“±
- ì¼ë°˜ í…ìŠ¤íŠ¸: ë‹¨ìˆœ ë¬¸ì¥

12GB GPU (RTX 5070 Ti) ìµœì í™”
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging
from pathlib import Path
from typing import Optional, Dict
import re

logger = logging.getLogger(__name__)


class Qwen3Translator:
    """Qwen3-8b LoRA ê¸°ë°˜ ë²ˆì—­ ëª¨ë¸ (ìë™ í˜•ì‹ ê°ì§€)"""
    
    # 12GB GPU ìµœì í™” ì„¤ì •
    MAX_INPUT_LENGTH = 4096
    MAX_OUTPUT_CAP = 4096
    MIN_OUTPUT_TOKENS = 512
    
    def __init__(
        self,
        model_path: str,
        use_gpu: bool = True,
        load_in_4bit: bool = True
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: LoRA ëª¨ë¸ ê²½ë¡œ
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            load_in_4bit: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
        """
        self.model_path = Path(model_path)
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.load_in_4bit = load_in_4bit
        
        self.model = None
        self.tokenizer = None
        
        logger.info(f"Qwen3Translator ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
        if self.use_gpu:
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"GPU ë©”ëª¨ë¦¬: {total_memory:.2f} GB")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë”©"""
        if self.model is not None:
            logger.warning("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info(f"ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
            
            # Tokenizer
            logger.info("Tokenizer ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            # 4bit ì–‘ìí™”
            if self.load_in_4bit and self.use_gpu:
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                )
                
                logger.info("4bit ì–‘ìí™” í™œì„±í™”")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.use_gpu else torch.float32
                )
            
            self.model.eval()
            
            logger.info("âœ… ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            logger.info(f"   4bit ì–‘ìí™”: {self.load_in_4bit}")
            
            if self.use_gpu:
                allocated = torch.cuda.memory_allocated() / 1e9
                logger.info(f"   ëª¨ë¸ ë©”ëª¨ë¦¬: {allocated:.2f} GB")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def translate(
        self,
        text: str,
        source_lang: str = "ko",
        target_lang: str = "ja",
        max_new_tokens: Optional[int] = None,
        temperature: float = 0.3,  #1ì¼ìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ê³  0ì¼ìˆ˜ë¡ 
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> Dict[str, str]:
        """
        í…ìŠ¤íŠ¸ ë²ˆì—­ (ğŸ¯ ìë™ í˜•ì‹ ê°ì§€)
        
        ìë™ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì„ ê°ì§€í•˜ê³  ì²˜ë¦¬:
        - íŠ¸ëœìŠ¤í¬ë¦½íŠ¸: [mm:ss] í™”ì: ë‚´ìš© â†’ íƒ€ì„ìŠ¤íƒ¬í”„+í™”ì ë³´ì¡´
        - ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸: ê°€ì‚¬, ì‹œ ë“± â†’ ì¤„ ë‹¨ìœ„ ë²ˆì—­
        - ì¼ë°˜ í…ìŠ¤íŠ¸: ë‹¨ìˆœ ë¬¸ì¥ â†’ ì¼ë°˜ ë²ˆì—­
        
        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            source_lang: ì›ë³¸ ì–¸ì–´ (ko, ja, en)
            target_lang: ëª©í‘œ ì–¸ì–´ (ko, ja, en)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜ (Noneì´ë©´ ìë™ ê³„ì‚°)
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_p: nucleus sampling
            do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            {
                "original_text": str,
                "translated_text": str,
                "source_lang": str,
                "target_lang": str,
                "input_tokens": int,
                "output_tokens": int
            }
        """
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # í˜•ì‹ ìë™ ê°ì§€
        lines = text.strip().split('\n')
        
        # íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ í˜•ì‹ ê°ì§€: [íƒ€ì„ìŠ¤íƒ¬í”„] í™”ì: ë‚´ìš©
        transcript_pattern = r'^(\[[\d:\.]+\])?\s*(í™”ì\d+|Speaker\d+|[^:]+):\s*.+$'
        first_line = lines[0].strip() if lines else ""
        is_transcript = bool(re.match(transcript_pattern, first_line))
        
        # ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸ (3ì¤„ ì´ìƒ)
        is_multiline = len(lines) >= 3
        
        # ìë™ ë¼ìš°íŒ…
        if is_transcript:
            logger.info("ğŸ“‹ ìë™ ê°ì§€: íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ í˜•ì‹")
            return self._translate_transcript(
                text, source_lang, target_lang,
                temperature, top_p, do_sample
            )
        elif is_multiline:
            logger.info("ğŸ“ ìë™ ê°ì§€: ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸")
            return self._translate_multiline(
                text, source_lang, target_lang,
                max_new_tokens, temperature, top_p, do_sample
            )
        else:
            logger.info("ğŸ’¬ ìë™ ê°ì§€: ì¼ë°˜ í…ìŠ¤íŠ¸")
            return self._translate_single(
                text, source_lang, target_lang,
                max_new_tokens, temperature, top_p, do_sample
            )
    
    def _translate_single(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        max_new_tokens: Optional[int],
        temperature: float,
        top_p: float,
        do_sample: bool
    ) -> Dict[str, str]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        
        try:
            logger.info(f"ë²ˆì—­ ì‹œì‘: {source_lang} â†’ {target_lang}")
            logger.info(f"ì›ë¬¸ ê¸¸ì´: {len(text)} ê¸€ì")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(text, source_lang, target_lang)
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.MAX_INPUT_LENGTH
            ).to(self.device)
            
            input_length = inputs['input_ids'].shape[1]
            logger.info(f"ì…ë ¥ í† í° ê¸¸ì´: {input_length}")
            
            # max_new_tokens ë™ì  ê³„ì‚°
            if max_new_tokens is None:
                calculated_tokens = int(input_length * 1.5) + 200
                max_new_tokens = min(
                    max(calculated_tokens, self.MIN_OUTPUT_TOKENS),
                    self.MAX_OUTPUT_CAP
                )
                logger.info(f"âœ… ë™ì  max_new_tokens: {max_new_tokens}")
            else:
                max_new_tokens = min(max_new_tokens, self.MAX_OUTPUT_CAP)
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            if self.use_gpu:
                free_memory = (
                    torch.cuda.get_device_properties(0).total_memory 
                    - torch.cuda.memory_allocated()
                )
                free_gb = free_memory / 1e9
                logger.info(f"ì—¬ìœ  VRAM: {free_gb:.2f} GB")
                
                if free_gb < 2.0:
                    logger.warning("âš ï¸ VRAM ë¶€ì¡± ê²½ê³ !")
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=3
                )
            
            output_length = outputs.shape[1]
            actual_generated = output_length - input_length
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
            translated_text = self._extract_translation(generated_text, prompt)
            
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {len(translated_text)} ê¸€ì")
            
            if self.use_gpu:
                torch.cuda.empty_cache()
            
            return {
                "original_text": text,
                "translated_text": translated_text,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "input_tokens": input_length,
                "output_tokens": actual_generated
            }
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±!")
                if self.use_gpu:
                    torch.cuda.empty_cache()
                raise RuntimeError("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. í…ìŠ¤íŠ¸ë¥¼ ë” ì§§ê²Œ ë‚˜ëˆ„ì„¸ìš”.") from e
            raise
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            raise
    
    def _translate_multiline(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        max_new_tokens: Optional[int],
        temperature: float,
        top_p: float,
        do_sample: bool
    ) -> Dict[str, str]:
        """ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸ ë²ˆì—­ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        lines = text.strip().split('\n')
        translated_lines = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        logger.info(f"ì¤„ ë‹¨ìœ„ ë²ˆì—­: {len(lines)}ì¤„")
        
        for i, line in enumerate(lines, 1):
            if line.strip():
                try:
                    result = self._translate_single(
                        text=line.strip(),
                        source_lang=source_lang,
                        target_lang=target_lang,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=do_sample
                    )
                    translated_lines.append(result['translated_text'])
                    total_input_tokens += result['input_tokens']
                    total_output_tokens += result['output_tokens']
                    logger.debug(f"  {i}/{len(lines)} ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"  {i}ë²ˆì§¸ ì¤„ ì‹¤íŒ¨: {e}")
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨: {line}]")
            else:
                translated_lines.append('')
        
        logger.info(f"âœ… ì¤„ ë‹¨ìœ„ ë²ˆì—­ ì™„ë£Œ: {len(lines)}ì¤„")
        
        return {
            "original_text": text,
            "translated_text": '\n'.join(translated_lines),
            "source_lang": source_lang,
            "target_lang": target_lang,
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens
        }
    
    def _translate_transcript(
        self,
        text: str,
        source_lang: str,
        target_lang: str,
        temperature: float,
        top_p: float,
        do_sample: bool
    ) -> Dict[str, str]:
        """íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        lines = text.strip().split('\n')
        translated_lines = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        # ì •ê·œí‘œí˜„ì‹ íŒ¨í„´: [íƒ€ì„ìŠ¤íƒ¬í”„] í™”ì: ë‚´ìš©
        pattern = r'^(\[[\d:\.]+\])?\s*(í™”ì\d+|Speaker\d+|[^:]+):\s*(.+)$'
        
        logger.info(f"íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­: {len(lines)}ì¤„")
        
        for i, line in enumerate(lines, 1):
            line = line.strip()
            
            if not line:
                translated_lines.append('')
                continue
            
            match = re.match(pattern, line)
            
            if match:
                timestamp = match.group(1) or ''
                speaker = match.group(2)
                content = match.group(3)
                
                try:
                    # ë‚´ìš©ë§Œ ë²ˆì—­
                    result = self._translate_single(
                        text=content.strip(),
                        source_lang=source_lang,
                        target_lang=target_lang,
                        max_new_tokens=None,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=do_sample
                    )
                    
                    # ì¬ì¡°ë¦½
                    if timestamp:
                        reconstructed = f"{timestamp} {speaker}: {result['translated_text']}"
                    else:
                        reconstructed = f"{speaker}: {result['translated_text']}"
                    
                    translated_lines.append(reconstructed)
                    total_input_tokens += result['input_tokens']
                    total_output_tokens += result['output_tokens']
                    logger.debug(f"  {i}/{len(lines)} [{speaker}] ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"  {i}ë²ˆì§¸ ì¤„ ì‹¤íŒ¨: {e}")
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨] {line}")
            else:
                # íŒ¨í„´ ë¶ˆì¼ì¹˜ - ì¼ë°˜ ë²ˆì—­
                try:
                    result = self._translate_single(
                        text=line,
                        source_lang=source_lang,
                        target_lang=target_lang,
                        max_new_tokens=None,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=do_sample
                    )
                    translated_lines.append(result['translated_text'])
                    total_input_tokens += result['input_tokens']
                    total_output_tokens += result['output_tokens']
                except Exception as e:
                    translated_lines.append(f"[ë²ˆì—­ ì‹¤íŒ¨] {line}")
        
        logger.info(f"âœ… íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë²ˆì—­ ì™„ë£Œ: {len(lines)}ì¤„")
        
        return {
            "original_text": text,
            "translated_text": '\n'.join(translated_lines),
            "source_lang": source_lang,
            "target_lang": target_lang,
            "input_tokens": total_input_tokens,
            "output_tokens": total_output_tokens
        }
    
    def _create_prompt(self, text: str, source_lang: str, target_lang: str) -> str:
        """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        lang_map = {
            "ko": "Korean",
            "ja": "Japanese", 
            "en": "English"
        }
        
        source_full = lang_map.get(source_lang.lower(), source_lang)
        target_full = lang_map.get(target_lang.lower(), target_lang)
        direction = f"[{source_full} to {target_full}]"
        
        # ê°•í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_content = (
            "You are a professional Korean-Japanese bilingual translator. "
            "Translate ONLY the given text accurately without any explanations, "
            "notes, or additional content. Do not mix other languages. "
            "Output only the translated text."
        )
        
        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user", 
                "content": f"{direction}\n{text}"
            }
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        
        return prompt

    def _extract_translation(self, generated_text: str, prompt: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶”ì¶œ"""
        result = generated_text
        
        # 1. í”„ë¡¬í”„íŠ¸ ì œê±°
        if prompt in result:
            result = result.replace(prompt, "").strip()
        
        # 2. special tokens ì œê±°
        special_tokens = [
            "<|im_start|>", "<|im_end|>", "<|endoftext|>",
            "system\n", "user\n", "assistant\n",
            "<|system|>", "<|user|>", "<|assistant|>"
        ]
        
        for token in special_tokens:
            result = result.replace(token, "")
        
        # 3. thinking íƒœê·¸ ì œê±°
        if "<think>" in result:
            result = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL)
        
        # 4. ì„¤ëª… íŒ¨í„´ ì œê±° (ì˜ˆ: "...ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤", "...ë¥¼ ëœ»í•©ë‹ˆë‹¤")
        explanation_patterns = [
            r'[.ã€‚]?\s*ì´\s*ë¬¸ì¥[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ë¼ëŠ”\s*ì˜ë¯¸[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ë¥¼\s*ëœ»[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ì›ë¬¸ì˜\s*ë§¥ë½[^.ã€‚]*[.ã€‚]',
            r'[.ã€‚]?\s*ìì—°ìŠ¤ëŸ½ê²Œ\s*í‘œí˜„í•˜ë©´[^.ã€‚]*[.ã€‚]',
            r'\*\*[^*]+\*\*',  # **êµµì€ ê¸€ì”¨** ì œê±°
            r'ë˜ëŠ”\s*\n',       # "ë˜ëŠ”" ë’¤ì˜ ì¶”ê°€ ì„¤ëª… ì œê±°
        ]
        
        for pattern in explanation_patterns:
            result = re.sub(pattern, '', result, flags=re.IGNORECASE)
        
        # 5. ë‹¤ë¥¸ ì–¸ì–´ ì„ì„ ê°ì§€ ë° ì œê±° (ì˜ì–´ ë‹¨ì–´ ë©ì–´ë¦¬, ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜)
        # ì˜ì–´ ë‹¨ì–´ê°€ ë§ì´ ì„ì˜€ìœ¼ë©´ ë¬¸ì œ
        english_ratio = len(re.findall(r'[a-zA-Z]{3,}', result)) / max(len(result.split()), 1)
        if english_ratio > 0.3:  # 30% ì´ìƒ ì˜ì–´ë©´ ì˜ì‹¬
            logger.warning(f"ì˜ì–´ ë¹„ìœ¨ ë†’ìŒ: {english_ratio:.2%}")
        
        # 6. ì¤„ ë‹¨ìœ„ë¡œ ë¼ë²¨ ì œê±°
        lines = result.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            # ì‹œìŠ¤í…œ ë¼ë²¨ ì œê±°
            if line.lower() in ['system', 'user', 'assistant']:
                continue
            # ë„ˆë¬´ ì§§ê±°ë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆìœ¼ë©´ ì œê±°
            if len(line) > 0 and not line.replace(' ', '').replace('*', '').replace('-', ''):
                continue
            if line:
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines)
        result = result.strip()
        
        # 7. ìµœì¢… ê²€ì¦
        if not result:
            logger.warning("âš ï¸ ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            logger.debug(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text[:200]}...")
    
        return result
    
    def get_memory_stats(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if not self.use_gpu:
            return {"message": "GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
        
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        free = total - allocated
        
        return {
            "total_gb": round(total, 2),
            "allocated_gb": round(allocated, 2),
            "reserved_gb": round(reserved, 2),
            "free_gb": round(free, 2)
        }
    
    def unload_model(self):
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            
            if self.use_gpu:
                torch.cuda.empty_cache()
                logger.info("GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
            logger.info("ë²ˆì—­ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
qwen3_translator: Optional[Qwen3Translator] = None


def initialize_translator(
    model_path: str = "qwen3-8b-lora-10ratio",
    use_gpu: bool = True,
    load_in_4bit: bool = True
):
    """ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™”"""
    global qwen3_translator
    
    logger.info("="*50)
    logger.info("ğŸš€ ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
    logger.info("="*50)
    
    qwen3_translator = Qwen3Translator(
        model_path=model_path,
        use_gpu=use_gpu,
        load_in_4bit=load_in_4bit
    )
    qwen3_translator.load_model()
    
    logger.info("="*50)
    logger.info("âœ… ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    logger.info("="*50)


def get_memory_info() -> Dict[str, float]:
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""
    if qwen3_translator is None:
        return {"error": "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
    return qwen3_translator.get_memory_stats()