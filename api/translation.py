"""
Qwen3-8b LoRA ë²ˆì—­ ëª¨ë“ˆ (API í†µí•© ë²„ì „)
ìœ„ì¹˜: api/translation.py
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging
from pathlib import Path
from typing import Optional, Dict
import re

logger = logging.getLogger(__name__)


class Qwen3Translator:
    """Qwen3-8b LoRA ê¸°ë°˜ ë²ˆì—­ ëª¨ë¸"""
    
    def __init__(
        self,
        model_path: str,
        use_gpu: bool = True,
        load_in_4bit: bool = True
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: LoRA ëª¨ë¸ ê²½ë¡œ
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            load_in_4bit: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
        """
        self.model_path = Path(model_path)
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.load_in_4bit = load_in_4bit
        
        self.model = None
        self.tokenizer = None
        
        logger.info(f"Qwen3Translator ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë”©"""
        if self.model is not None:
            logger.warning("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info(f"ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")
            
            # Tokenizer
            logger.info("Tokenizer ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True
            )
            
            # 4bit ì–‘ìí™”
            if self.load_in_4bit and self.use_gpu:
                from transformers import BitsAndBytesConfig
                
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
                )
                
                logger.info("4bit ì–‘ìí™” í™œì„±í™”")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.use_gpu else torch.float32
                )
            
            self.model.eval()
            
            logger.info("âœ… ë²ˆì—­ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            logger.info(f"   4bit ì–‘ìí™”: {self.load_in_4bit}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def translate(
        self,
        text: str,
        source_lang: str = "ko",
        target_lang: str = "ja",
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> Dict[str, str]:
        """
        í…ìŠ¤íŠ¸ ë²ˆì—­
        
        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸
            source_lang: ì›ë³¸ ì–¸ì–´ (ko, ja, en)
            target_lang: ëª©í‘œ ì–¸ì–´ (ko, ja, en)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_p: nucleus sampling
            do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            {
                "original_text": str,
                "translated_text": str,
                "source_lang": str,
                "target_lang": str
            }
        """
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        try:
            logger.info(f"ë²ˆì—­ ì‹œì‘: {source_lang} â†’ {target_lang}")
            logger.info(f"ì›ë¬¸: {text[:100]}...")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(text, source_lang, target_lang)
            logger.debug(f"ìƒì„±ëœ í”„ë¡¬í”„íŠ¸:\n{prompt}")
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                    no_repeat_ngram_size=3   # n-gram ë°˜ë³µ ë°©ì§€
                )
            
            # ë””ì½”ë”© - skip_special_tokens=Falseë¡œ í•´ì„œ ìˆ˜ë™ ì œê±°
            generated_text = self.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=False
            )
            
            logger.debug(f"ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸:\n{generated_text}")
            
            # ë²ˆì—­ ê²°ê³¼ ì¶”ì¶œ
            translated_text = self._extract_translation(generated_text, prompt)
            
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {translated_text[:100]}...")
            
            return {
                "original_text": text,
                "translated_text": translated_text,
                "source_lang": source_lang,
                "target_lang": target_lang
            }
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì‹¤íŒ¨: {e}")
            raise
    
    def _create_prompt(self, text: str, source_lang: str, target_lang: str) -> str:
        """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ìƒì„± (í•™ìŠµ ë°ì´í„°ì™€ ì •í™•íˆ ë™ì¼í•œ í˜•ì‹)"""
        
        # ì–¸ì–´ ì½”ë“œ â†’ í’€ë„¤ì„ ë³€í™˜ (í•™ìŠµ ë°ì´í„°ì™€ ì¼ì¹˜!)
        lang_map = {
            "ko": "Korean",
            "ja": "Japanese", 
            "en": "English"
        }
        
        source_full = lang_map.get(source_lang.lower(), source_lang)
        target_full = lang_map.get(target_lang.lower(), target_lang)
        
        # í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ í˜•ì‹: [Japanese to Korean]
        direction = f"[{source_full} to {target_full}]"
        
        messages = [
            {
                "role": "system",
                "content": " You are a professional Korean-Japanese bilingual translator."
            },
            {
                "role": "user", 
                "content": f"{direction}\n{text}"
            }
        ]
        
        # Qwen3 í…œí”Œë¦¿ ì ìš© - thinking ë¹„í™œì„±í™” í•„ìˆ˜!
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # ì¤‘ìš”!
        )
        
        return prompt

    def _extract_translation(self, generated_text: str, prompt: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶”ì¶œ"""
        
        result = generated_text
        
        # 1. ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì „ì²´ ì œê±°
        if prompt in result:
            result = result.replace(prompt, "").strip()
        
        # 2. special tokens ì œê±° (ë””ì½”ë”© í›„ì—ë„ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ)
        special_tokens = [
            "<|im_start|>", "<|im_end|>", "<|endoftext|>",
            "system\n", "user\n", "assistant\n",
            "<|system|>", "<|user|>", "<|assistant|>"
        ]
        
        for token in special_tokens:
            result = result.replace(token, "")
        
        # 3. thinking íƒœê·¸ ì œê±° (<think>...</think>)
        if "<think>" in result:
            result = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL)
        
        # 4. system/user/assistant ë¼ë²¨ ì œê±° (ì¤„ ë‹¨ìœ„)
        lines = result.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            # system, user, assistantë¡œë§Œ ëœ ì¤„ ì œê±°
            if line.lower() in ['system', 'user', 'assistant']:
                continue
            if line:
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines)
        
        # 5. ì•ë’¤ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        result = result.strip()
        
        # 6. ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
        if not result or len(result) < 5:
            logger.warning(f"ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ìŠµë‹ˆë‹¤: '{result}'")
            logger.warning(f"ìƒì„±ëœ ì „ì²´ í…ìŠ¤íŠ¸: {generated_text}")
        
        return result
    
    def unload_model(self):
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache()
            logger.info("ë²ˆì—­ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
qwen3_translator: Optional[Qwen3Translator] = None


def initialize_translator(
    model_path: str = "qwen3-8b-lora-10ratio",
    use_gpu: bool = True,
    load_in_4bit: bool = True
):
    """
    ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™”
    
    Args:
        model_path: LoRA ëª¨ë¸ ê²½ë¡œ
        use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        load_in_4bit: 4bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
    """
    global qwen3_translator
    
    logger.info("="*50)
    logger.info("ğŸš€ ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
    logger.info("="*50)
    
    qwen3_translator = Qwen3Translator(
        model_path=model_path,
        use_gpu=use_gpu,
        load_in_4bit=load_in_4bit
    )
    qwen3_translator.load_model()
    
    logger.info("="*50)
    logger.info("âœ… ë²ˆì—­ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    logger.info("="*50)