"""
STT (Speech-to-Text) ì¶”ë¡  ëª¨ë“ˆ - API ì „ìš©
ì›ë³¸: audio_pipeline.pyì—ì„œ í•µì‹¬ ê¸°ëŠ¥ë§Œ ì¶”ì¶œ
"""

import torch
import torchaudio
import whisper
import logging
from pathlib import Path
import numpy as np
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)


class WhisperSTT:
    """
    Whisper ê¸°ë°˜ ìŒì„±ì¸ì‹ í´ë˜ìŠ¤ (API ìµœì í™” ë²„ì „)
    """
    
    def __init__(
        self, 
        model_size: str = "medium",
        use_gpu: bool = True,
        language: Optional[str] = None
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_size: Whisper ëª¨ë¸ í¬ê¸° ('tiny', 'base', 'small', 'medium', 'large', 'large-v3')
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
            language: íƒ€ê²Ÿ ì–¸ì–´ ì½”ë“œ (Noneì´ë©´ ìë™ ê°ì§€)
                     'en', 'ko', 'ja', 'zh', 'es', 'fr', 'de', 'ru' ë“±
        """
        self.model_size = model_size
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.device = "cuda" if self.use_gpu else "cpu"
        self.language = language
        self.model = None
        
        logger.info(f"WhisperSTT ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}, ëª¨ë¸: {model_size}")
        
        # ì§€ì› ì–¸ì–´ ì •ë³´
        self.supported_languages = {
            'en': 'English',
            'ko': 'í•œêµ­ì–´',
            'ja': 'æ—¥æœ¬èª',
            'zh': 'ä¸­æ–‡',
            'es': 'EspaÃ±ol',
            'fr': 'FranÃ§ais',
            'de': 'Deutsch',
            'ru': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹'
        }
    
    def load_model(self):
        """Whisper ëª¨ë¸ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ í˜¸ì¶œ)"""
        if self.model is None:
            try:
                logger.info(f"Whisper ëª¨ë¸ ({self.model_size}) ë¡œë”© ì¤‘...")
                self.model = whisper.load_model(self.model_size, device=self.device)
                logger.info("âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
    
    def transcribe(
        self, 
        audio_path: str,
        language: Optional[str] = None,
        word_timestamps: bool = False,
        initial_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì§€ì • (Noneì´ë©´ ìë™ ê°ì§€ ë˜ëŠ” ì´ˆê¸°í™” ì‹œ ì„¤ì •í•œ ì–¸ì–´)
            word_timestamps: ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ì—¬ë¶€
            initial_prompt: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ (ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
        
        Returns:
            {
                "text": str,  # ì „ì²´ í…ìŠ¤íŠ¸
                "segments": [...],  # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì •ë³´
                "language": str  # ê°ì§€/ì‚¬ìš©ëœ ì–¸ì–´
            }
        """
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        try:
            logger.info(f"ğŸ¤ STT ì²˜ë¦¬ ì‹œì‘: {audio_path}")
            
            # ì–¸ì–´ ì„¤ì • ìš°ì„ ìˆœìœ„: ë©”ì„œë“œ íŒŒë¼ë¯¸í„° > ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • > ìë™ ê°ì§€
            target_lang = language or self.language
            
            # Whisper transcribe ì˜µì…˜
            transcribe_options = {
                "word_timestamps": word_timestamps,
                "verbose": False
            }
            
            if target_lang:
                transcribe_options["language"] = target_lang
                lang_name = self.supported_languages.get(target_lang, target_lang)
                logger.info(f"ì–¸ì–´ ì§€ì •: {lang_name}")
            else:
                logger.info("ì–¸ì–´ ìë™ ê°ì§€ ëª¨ë“œ")
            
            if initial_prompt:
                transcribe_options["initial_prompt"] = initial_prompt
            
            # STT ìˆ˜í–‰
            result = self.model.transcribe(str(audio_path), **transcribe_options)
            
            transcribed_text = result["text"].strip()
            detected_language = result.get("language", "unknown")
            
            logger.info(f"âœ… STT ì™„ë£Œ - ì–¸ì–´: {detected_language}")
            logger.info(f"ğŸ“ í…ìŠ¤íŠ¸: {transcribed_text[:100]}...")
            
            return {
                "text": transcribed_text,
                "segments": result.get("segments", []),
                "language": detected_language
            }
            
        except Exception as e:
            logger.error(f"âŒ STT ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def transcribe_simple(
        self, 
        audio_path: str,
        language: Optional[str] = None
    ) -> str:
        """
        ê°„ë‹¨í•œ ë²„ì „ - í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì§€ì • (ì„ íƒì‚¬í•­)
        
        Returns:
            str: ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        result = self.transcribe(audio_path, language=language)
        return result["text"]
    
    def unload_model(self):
        """ë©”ëª¨ë¦¬ í•´ì œ"""
        if self.model is not None:
            try:
                logger.info("ğŸ”„ Whisper ëª¨ë¸ GPU ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...")
                
                # GPUì—ì„œ CPUë¡œ ì´ë™ (GPU ë©”ëª¨ë¦¬ í™•ë³´)
                if hasattr(self.model, 'to'):
                    self.model.to('cpu')
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ CPUë¡œ ëª…ì‹œì ìœ¼ë¡œ ì´ë™
                if hasattr(self.model, 'parameters'):
                    for param in self.model.parameters():
                        if param.is_cuda:
                            param.data = param.data.cpu()
                
                # ëª¨ë¸ì˜ ëª¨ë“  ë²„í¼ë¥¼ CPUë¡œ ì´ë™
                if hasattr(self.model, 'buffers'):
                    for buffer in self.model.buffers():
                        if buffer.is_cuda:
                            buffer.data = buffer.data.cpu()
                
                # ëª¨ë¸ ì‚­ì œ
                del self.model
                self.model = None
                
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰ (ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ì •ë¦¬)
                import gc
                gc.collect()
                gc.collect()
                gc.collect()  # ì„¸ ë²ˆì§¸ë¡œ í™•ì‹¤í•˜ê²Œ ì •ë¦¬
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ê°•ë ¥í•˜ê²Œ)
                if self.use_gpu and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()  # í•œ ë²ˆ ë”
                    try:
                        torch.cuda.reset_peak_memory_stats()
                        # CUDA IPC ë©”ëª¨ë¦¬ ì •ë¦¬ (ê³µìœ  ë©”ëª¨ë¦¬)
                        if hasattr(torch.cuda, 'ipc_collect'):
                            torch.cuda.ipc_collect()
                    except Exception:
                        pass
                    
                    # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…
                    allocated = torch.cuda.memory_allocated() / 1e9
                    reserved = torch.cuda.memory_reserved() / 1e9
                    logger.info(f"âœ… Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ (GPU í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB)")
                else:
                    logger.info("âœ… Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                self.model = None


class AudioDenoiser:
    """
    ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ ì œê±° í´ë˜ìŠ¤ (ì„ íƒì‚¬í•­)
    SpeechBrain ì—†ì´ë„ ì‘ë™í•˜ëŠ” ê°„ë‹¨í•œ í•„í„°
    """
    
    def __init__(self):
        self.target_sr = 16000
        logger.info("AudioDenoiser ì´ˆê¸°í™” (ê°„ë‹¨í•œ í•„í„°ë§)")
    
    def denoise(self, input_path: str, output_path: str):
        """
        ë…¸ì´ì¦ˆ ì œê±° ìˆ˜í–‰
        
        Args:
            input_path: ì…ë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ì˜¤ë””ì˜¤ ê²½ë¡œ
        """
        try:
            logger.info(f"ğŸ”§ ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘: {input_path}")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            waveform, sample_rate = torchaudio.load(input_path)
            
            # ëª¨ë…¸ë¡œ ë³€í™˜
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
            if sample_rate != self.target_sr:
                resampler = torchaudio.transforms.Resample(sample_rate, self.target_sr)
                waveform = resampler(waveform)
                sample_rate = self.target_sr
            
            # ê°„ë‹¨í•œ í•„í„°ë§ ì ìš©
            filtered_waveform = self._apply_simple_filter(waveform.squeeze(0), sample_rate)
            filtered_waveform = filtered_waveform.unsqueeze(0)
            
            # ì €ì¥
            torchaudio.save(output_path, filtered_waveform, sample_rate)
            
            logger.info(f"âœ… ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            logger.error(f"âŒ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨: {e}")
            raise
    
    def _apply_simple_filter(self, waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:
        """
        ê°„ë‹¨í•œ ìŠ¤í™íŠ¸ëŸ¼ í•„í„°ë§ ì ìš©
        
        Args:
            waveform: ì˜¤ë””ì˜¤ íŒŒí˜•
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        
        Returns:
            í•„í„°ë§ëœ íŒŒí˜•
        """
        try:
            from scipy.signal import butter, filtfilt
            
            # 80Hz ì´í•˜ ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±° (ê³ ì—­ í†µê³¼ í•„í„°)
            nyquist = sample_rate / 2
            low_cutoff = 80 / nyquist
            b, a = butter(4, low_cutoff, btype='high')
            
            # í•„í„° ì ìš©
            filtered = filtfilt(b, a, waveform.numpy())
            
            # ì •ê·œí™” (0.8ë°°ë¡œ ì•ˆì „ ë§ˆì§„)
            filtered = filtered / np.max(np.abs(filtered)) * 0.8
            
            return torch.from_numpy(filtered).float()
            
        except ImportError:
            logger.warning("scipy ë¯¸ì„¤ì¹˜ - ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ ì ìš©")
            # scipy ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì •ê·œí™”ë§Œ
            normalized = waveform / torch.max(torch.abs(waveform)) * 0.8
            return normalized


# ===== APIì—ì„œ ì‚¬ìš©í•  ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ =====
whisper_stt: Optional[WhisperSTT] = None
audio_denoiser: Optional[AudioDenoiser] = None


def initialize_stt_models(
    whisper_model_size: str = "medium",
    language: Optional[str] = None,
    use_denoiser: bool = False
):
    """
    STT ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œ)
    
    Args:
        whisper_model_size: Whisper ëª¨ë¸ í¬ê¸°
        language: ê¸°ë³¸ ì–¸ì–´ ì„¤ì •
        use_denoiser: ë…¸ì´ì¦ˆ ì œê±° ì‚¬ìš© ì—¬ë¶€
    """
    global whisper_stt, audio_denoiser
    
    logger.info("="*50)
    logger.info("ğŸš€ STT ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
    logger.info("="*50)
    
    # Whisper STT ì´ˆê¸°í™”
    whisper_stt = WhisperSTT(
        model_size=whisper_model_size,
        use_gpu=True,
        language=language
    )
    whisper_stt.load_model()
    
    # ë…¸ì´ì¦ˆ ì œê±° (ì„ íƒì‚¬í•­)
    if use_denoiser:
        audio_denoiser = AudioDenoiser()
        logger.info("âœ… ë…¸ì´ì¦ˆ ì œê±° í™œì„±í™”")
    
    logger.info("="*50)
    logger.info("âœ… STT ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
    logger.info("="*50)


class TranslateModel:
    def __init__(self, base_model_path, lora_path, s_lang, t_lang):
        pass
    def load_model(self):
        pass
    def translate(self, text: str, source_lang='s', target_lang='t', max_length=512):
        pass
    def unload_model(self):
        pass
    pass

def initialize_models(config):
    pass