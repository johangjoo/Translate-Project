"""
í•™ìŠµ ì†ë„ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
- GPU ì‚¬ìš©ë¥ , ë°°ì¹˜ í¬ê¸°, ìŠ¤í… ì‹œê°„ ë“± ëª¨ë“  ì •ë³´ ì¶œë ¥
- ì´ ì •ë³´ë¡œ ë³‘ëª© ì§€ì  íŒŒì•… ê°€ëŠ¥
"""
import torch
import time
import gc
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent

# ì„¤ì •
MAX_SEQ_LENGTH = 512
SAMPLE_RATIO = 0.10

MODEL_NAME = "Qwen/Qwen3-8b"
OUTPUT_DIR = str(PROJECT_ROOT / "qwen3-8b-translation-lora")
NEW_MODEL_NAME = "qwen3-8b-ko-ja-translation"

TRAIN_FILE = str(PROJECT_ROOT / "train.jsonl")
VAL_FILE = str(PROJECT_ROOT / "validation.jsonl")

print("\n" + "="*80)
print("  ğŸ” í•™ìŠµ ì†ë„ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸")
print("="*80 + "\n")

# ==========================
# 1. ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
# ==========================
print("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
print("-" * 80)

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"âœ… GPU: {gpu_name}")
    print(f"   VRAM: {gpu_memory:.1f} GB")
    print(f"   CUDA ë²„ì „: {torch.version.cuda}")
    print(f"   PyTorch ë²„ì „: {torch.__version__}")
else:
    print("âŒ GPU ì—†ìŒ - CPUë¡œ í•™ìŠµí•˜ë©´ ì—„ì²­ ëŠë¦½ë‹ˆë‹¤!")

# CPU/RAM ì •ë³´
try:
    import psutil
    ram_total = psutil.virtual_memory().total / 1024**3
    cpu_count = psutil.cpu_count()
    print(f"\nğŸ’» CPU: {cpu_count}ê°œ ì½”ì–´")
    print(f"   RAM: {ram_total:.1f} GB")
except:
    print("\nğŸ’» CPU/RAM ì •ë³´ í™•ì¸ ë¶ˆê°€ (psutil ì„¤ì¹˜ í•„ìš”)")

print()

# ==========================
# 2. ëª¨ë¸ ë¡œë“œ & ì‹œê°„ ì¸¡ì •
# ==========================
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
start_time = time.time()

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

load_time = time.time() - start_time
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.1f}ì´ˆ)\n")

# ==========================
# 3. LoRA ì ìš©
# ==========================
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                    "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print()

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# ==========================
# 4. ë°ì´í„° ë¡œë“œ & ë¶„ì„
# ==========================
print("ğŸ“š ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘...")
start_time = time.time()

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        if input_text:
            text = f"""<|im_start|>system
You are a professional translator specializing in Korean-Japanese translation.<|im_end|>
<|im_start|>user
{instruction}

{input_text}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""
        else:
            text = f"""<|im_start|>system
You are a professional translator specializing in Korean-Japanese translation.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{output}<|im_end|>"""
        texts.append(text)
    return {"text": texts}

dataset = load_dataset(
    "json", 
    data_files={
        "train": TRAIN_FILE,
        "validation": VAL_FILE
    },
    keep_in_memory=False
)

print(f"   ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
print(f"   ì›ë³¸ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

# ìƒ˜í”Œë§
if SAMPLE_RATIO < 1.0:
    train_size = int(len(dataset['train']) * SAMPLE_RATIO)
    val_size = int(len(dataset['validation']) * SAMPLE_RATIO)
    
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(train_size))
    dataset["validation"] = dataset["validation"].shuffle(seed=42).select(range(val_size))
    
    print(f"\n   ìƒ˜í”Œë§ í›„ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
    print(f"   ìƒ˜í”Œë§ í›„ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

# ë°ì´í„° ê¸¸ì´ ë¶„ì„ (ìƒ˜í”Œ 1000ê°œ)
print("\nğŸ” ë°ì´í„° ê¸¸ì´ ë¶„ì„ ì¤‘ (ìƒ˜í”Œ 1000ê°œ)...")
sample_size = min(1000, len(dataset['train']))
lengths = []

for i in range(sample_size):
    example = dataset['train'][i]
    formatted = formatting_prompts_func({
        "instruction": [example["instruction"]],
        "input": [example["input"]],
        "output": [example["output"]]
    })
    tokens = tokenizer(formatted["text"][0], truncation=False)
    lengths.append(len(tokens['input_ids']))

import numpy as np
print(f"   í‰ê·  ê¸¸ì´: {np.mean(lengths):.0f} í† í°")
print(f"   ì¤‘ì•™ê°’: {np.median(lengths):.0f} í† í°")
print(f"   ìµœì†Œ: {np.min(lengths)} í† í°")
print(f"   ìµœëŒ€: {np.max(lengths)} í† í°")
print(f"   95ë°±ë¶„ìœ„: {np.percentile(lengths, 95):.0f} í† í°")
print(f"   99ë°±ë¶„ìœ„: {np.percentile(lengths, 99):.0f} í† í°")

over_512 = sum(1 for l in lengths if l > 512)
print(f"\n   512 í† í° ì´ˆê³¼: {over_512}/{sample_size} ({over_512/sample_size*100:.1f}%)")
print(f"   í˜„ì¬ MAX_LENGTH: {MAX_SEQ_LENGTH}")

if np.percentile(lengths, 95) < MAX_SEQ_LENGTH * 0.7:
    recommended = int(np.percentile(lengths, 95) * 1.1)
    print(f"   âš ï¸  ì¶”ì²œ MAX_LENGTH: {recommended} (í˜„ì¬ë³´ë‹¤ ì§§ê²Œ ì„¤ì • ê°€ëŠ¥)")

print()

# í¬ë§· ì ìš©
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
    batch_size=500,
    remove_columns=dataset["train"].column_names,
)

data_load_time = time.time() - start_time
print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ({data_load_time:.1f}ì´ˆ)\n")

gc.collect()

# ==========================
# 5. í•™ìŠµ ì„¤ì • ì§„ë‹¨
# ==========================
print("âš™ï¸  í•™ìŠµ ì„¤ì • ë¶„ì„:")
print("-" * 80)

# ì—¬ê¸°ì„œ ì‹¤ì œ ì‚¬ìš©ì ì„¤ì •ì„ í™•ì¸
PER_DEVICE_BATCH = 4  # ì‹¤ì œ ì½”ë“œì˜ ê°’
GRAD_ACCUM = 4        # ì‹¤ì œ ì½”ë“œì˜ ê°’
NUM_EPOCHS = 2

effective_batch = PER_DEVICE_BATCH * GRAD_ACCUM
total_samples = len(dataset['train']) * NUM_EPOCHS
total_steps_no_pack = total_samples // effective_batch

print(f"í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
print(f"ì—í­ ìˆ˜: {NUM_EPOCHS}")
print(f"Per-device ë°°ì¹˜: {PER_DEVICE_BATCH}")
print(f"Gradient accumulation: {GRAD_ACCUM}")
print(f"ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸°: {effective_batch}")
print(f"\níŒ¨í‚¹ ì—†ì„ ë•Œ:")
print(f"   ì´ ìŠ¤í…: {total_steps_no_pack:,}")
print(f"   ìŠ¤í…ë‹¹ 3ì´ˆ ê°€ì •: {total_steps_no_pack * 3 / 3600:.1f}ì‹œê°„")

# íŒ¨í‚¹ íš¨ê³¼ ì¶”ì •
avg_tokens = np.mean(lengths)
packing_efficiency = min(MAX_SEQ_LENGTH / avg_tokens, 2.0)  # ìµœëŒ€ 2ë°°
total_steps_with_pack = int(total_steps_no_pack / packing_efficiency)

print(f"\níŒ¨í‚¹ ìˆì„ ë•Œ (ì˜ˆìƒ):")
print(f"   í‰ê·  ê¸¸ì´: {avg_tokens:.0f} í† í°")
print(f"   íŒ¨í‚¹ íš¨ìœ¨: {packing_efficiency:.2f}ë°°")
print(f"   ì˜ˆìƒ ìŠ¤í…: {total_steps_with_pack:,}")
print(f"   ìŠ¤í…ë‹¹ 3ì´ˆ ê°€ì •: {total_steps_with_pack * 3 / 3600:.1f}ì‹œê°„")

print()

# ==========================
# 6. ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
# ==========================
print("ğŸ”¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ (3ê°œ ë°°ì¹˜):")
print("-" * 80)
print("ì‹¤ì œ í•™ìŠµ ì†ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤...\n")

sft_args = SFTConfig(
    output_dir=OUTPUT_DIR,
    run_name=NEW_MODEL_NAME,
    
    per_device_train_batch_size=PER_DEVICE_BATCH,
    gradient_accumulation_steps=GRAD_ACCUM,
    per_device_eval_batch_size=2,
    
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_steps=10,
    
    num_train_epochs=1,
    max_steps=3,  # 3 ìŠ¤í…ë§Œ í…ŒìŠ¤íŠ¸
    
    eval_strategy="no",
    save_strategy="no",
    logging_steps=1,
    
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    
    optim="adamw_8bit",
    weight_decay=0.01,
    max_grad_norm=1.0,
    seed=42,
    report_to="none",
    
    dataset_text_field="text",
    max_length=MAX_SEQ_LENGTH,
    packing=True,  # íŒ¨í‚¹ ìƒíƒœ í…ŒìŠ¤íŠ¸
    
    dataloader_num_workers=0,
    dataloader_pin_memory=False,
    
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
)

trainer = SFTTrainer(
    model=model,
    args=sft_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    processing_class=tokenizer,
)

print("â±ï¸  3 ìŠ¤í… í…ŒìŠ¤íŠ¸ ì¤‘...")
if torch.cuda.is_available():
    torch.cuda.reset_peak_memory_stats()
    start_mem = torch.cuda.memory_allocated() / 1024**3

benchmark_start = time.time()
trainer.train()
benchmark_time = time.time() - benchmark_start

if torch.cuda.is_available():
    peak_mem = torch.cuda.max_memory_allocated() / 1024**3
    current_mem = torch.cuda.memory_allocated() / 1024**3

avg_step_time = benchmark_time / 3

print(f"\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
print(f"   3 ìŠ¤í… ì´ ì‹œê°„: {benchmark_time:.1f}ì´ˆ")
print(f"   í‰ê·  ìŠ¤í… ì‹œê°„: {avg_step_time:.2f}ì´ˆ/ìŠ¤í…")

if torch.cuda.is_available():
    print(f"\n   GPU ë©”ëª¨ë¦¬:")
    print(f"   - í˜„ì¬ ì‚¬ìš©: {current_mem:.2f} GB")
    print(f"   - ìµœëŒ€ ì‚¬ìš©: {peak_mem:.2f} GB")
    print(f"   - ì—¬ìœ  ê³µê°„: {gpu_memory - peak_mem:.2f} GB")

# ==========================
# 7. ìµœì¢… ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
# ==========================
print("\n" + "="*80)
print("ğŸ“Š ìµœì¢… ì˜ˆìƒ ì‹œê°„")
print("="*80)

print(f"\níŒ¨í‚¹ ì—†ì´ í•™ìŠµ ì‹œ:")
print(f"   ì´ ìŠ¤í…: {total_steps_no_pack:,}")
print(f"   ìŠ¤í…ë‹¹ ì‹œê°„: {avg_step_time:.2f}ì´ˆ")
print(f"   ì˜ˆìƒ ì´ ì‹œê°„: {total_steps_no_pack * avg_step_time / 3600:.1f}ì‹œê°„")

print(f"\níŒ¨í‚¹ ìˆê²Œ í•™ìŠµ ì‹œ (í˜„ì¬ ì„¤ì •):")
print(f"   ì˜ˆìƒ ìŠ¤í…: {total_steps_with_pack:,}")
print(f"   ìŠ¤í…ë‹¹ ì‹œê°„: {avg_step_time:.2f}ì´ˆ")
print(f"   ì˜ˆìƒ ì´ ì‹œê°„: {total_steps_with_pack * avg_step_time / 3600:.1f}ì‹œê°„")

# ==========================
# 8. ë³‘ëª© ì§€ì  ë¶„ì„
# ==========================
print("\n" + "="*80)
print("ğŸ” ë³‘ëª© ì§€ì  ë¶„ì„")
print("="*80)

issues = []
recommendations = []

# GPU ì²´í¬
if not torch.cuda.is_available():
    issues.append("âŒ CRITICAL: GPUê°€ ì—†ìŠµë‹ˆë‹¤! CPUë¡œ í•™ìŠµí•˜ë©´ 100ë°° ì´ìƒ ëŠë¦½ë‹ˆë‹¤.")
    recommendations.append("â†’ GPUê°€ ìˆëŠ” í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")
elif "GTX" in gpu_name or "MX" in gpu_name:
    issues.append(f"âš ï¸  GPU ì„±ëŠ¥ ë‚®ìŒ: {gpu_name}")
    recommendations.append("â†’ RTX 3060 ì´ìƒ ê¶Œì¥")

# ë°°ì¹˜ í¬ê¸° ì²´í¬
if PER_DEVICE_BATCH <= 2:
    issues.append(f"âš ï¸  ë°°ì¹˜ í¬ê¸° ì‘ìŒ: {PER_DEVICE_BATCH}")
    if torch.cuda.is_available() and (gpu_memory - peak_mem) > 4:
        recommendations.append(f"â†’ per_device_train_batch_sizeë¥¼ 4-6ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš” (VRAM ì—¬ìœ : {gpu_memory - peak_mem:.1f}GB)")

# ì‹œí€€ìŠ¤ ê¸¸ì´ ì²´í¬
if np.percentile(lengths, 95) < MAX_SEQ_LENGTH * 0.7:
    recommended_length = int(np.percentile(lengths, 95) * 1.1)
    issues.append(f"âš ï¸  MAX_LENGTHê°€ ë„ˆë¬´ í¼: {MAX_SEQ_LENGTH} (95%ê°€ {np.percentile(lengths, 95):.0f} ì´í•˜)")
    recommendations.append(f"â†’ MAX_SEQ_LENGTHë¥¼ {recommended_length}ë¡œ ì¤„ì´ë©´ ë” ë¹ ë¦„")

# ìŠ¤í… ì‹œê°„ ì²´í¬
if avg_step_time > 5:
    issues.append(f"âš ï¸  ìŠ¤í…ì´ ë„ˆë¬´ ëŠë¦¼: {avg_step_time:.2f}ì´ˆ/ìŠ¤í…")
    recommendations.append("â†’ GPU ì‚¬ìš©ë¥  í™•ì¸ í•„ìš” (nvidia-smi ë˜ëŠ” ì‘ì—… ê´€ë¦¬ì)")
    
if avg_step_time < 1:
    issues.append(f"âœ… ìŠ¤í… ì†ë„ ë§¤ìš° ë¹ ë¦„: {avg_step_time:.2f}ì´ˆ/ìŠ¤í…")

# ì›Œì»¤ ì²´í¬
if sft_args.dataloader_num_workers == 0:
    recommendations.append("â†’ dataloader_num_workersë¥¼ 1-2ë¡œ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŒ (RAM ëª¨ë‹ˆí„°ë§ í•„ìš”)")

print()
if not issues:
    print("âœ… í° ë¬¸ì œ ì—†ìŒ!")
else:
    for issue in issues:
        print(issue)

print("\nğŸ’¡ ìµœì í™” ì œì•ˆ:")
if not recommendations:
    print("   í˜„ì¬ ì„¤ì •ì´ ì ì ˆí•©ë‹ˆë‹¤.")
else:
    for rec in recommendations:
        print(rec)

print("\n" + "="*80)
print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
print("="*80)
print("\n1. ìœ„ ì •ë³´ë¥¼ ì „ë¶€ ë³µì‚¬í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”")
print("2. íŠ¹íˆ ë‹¤ìŒ ì •ë³´ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤:")
print("   - GPU ì´ë¦„ê³¼ VRAM")
print("   - í‰ê·  ìŠ¤í… ì‹œê°„")
print("   - ì˜ˆìƒ ì´ ì‹œê°„")
print("   - ë³‘ëª© ì§€ì  ë¶„ì„ ê²°ê³¼")
print("\n3. í˜„ì¬ í•™ìŠµì´ ì‹¤ì œë¡œ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ëŠ”ì§€ë„ ì•Œë ¤ì£¼ì„¸ìš”")
print("   (20ì‹œê°„ì´ë¼ê³  í•˜ì…¨ëŠ”ë°, TensorBoardë‚˜ ë¡œê·¸ì—ì„œ í™•ì¸í•œ ê°’ì¸ê°€ìš”?)")
print("\n" + "="*80)