 Qwen 3 (8B) LoRA ë²ˆì—­ ëª¨ë¸ í•™ìŠµ


---

í”„ë¡œì íŠ¸ êµ¬ì¡°

```
project/
â”œâ”€â”€ LoraScript/                    # ğŸ“¦ ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼
â”‚   â”œâ”€â”€ setup.sh                   # í™˜ê²½ ì„¤ì •
â”‚   â”œâ”€â”€ requirements.txt           # í•„ìˆ˜ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ prepare_dataset.py         # ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ train_qwen_lora.py         # í•™ìŠµ
â”‚   â””â”€â”€ README.md
               
â”œâ”€â”€ training_data.jsonl            # ì „ì²˜ë¦¬ëœ ì „ì²´ ë°ì´í„°
â”œâ”€â”€ train.jsonl                    # í•™ìŠµìš©
â”œâ”€â”€ validation.jsonl               # ê²€ì¦ìš© gitì—ëŠ” ë”°ë¡œ í¬í•¨ x
â”‚
â””â”€â”€ qwen3-8b-@@@@/      
    â”œâ”€â”€ lora_adapters/            
    â””â”€â”€ qwen3-8b-@@@@/ # ìµœì¢… ëª¨ë¸
```

---

`train_qwen_lora.py` ìˆ˜ì •:
```python
per_device_train_batch_size=1    # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
gradient_accumulation_steps=8     # ëˆ„ì  ìŠ¤í… ëŠ˜ë¦¬ê¸°
LORA_R = 8                        # LoRA rank ì¤„ì´ê¸°
MAX_SEQ_LENGTH = 1024             # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
```

### ë” ë‚˜ì€ í’ˆì§ˆ

```python
num_train_epochs=5                # ì—í­ ëŠ˜ë¦¬ê¸°
LORA_R = 32                       # LoRA rank ëŠ˜ë¦¬ê¸°
learning_rate=1e-4                # í•™ìŠµë¥  ë‚®ì¶”ê¸°
```

--

## ğŸ“ í•™ìŠµ í›„ í™œìš©

### API ì„œë²„ë¡œ ë°°í¬
- FastAPIë¡œ REST API êµ¬ì¶•
- vLLMìœ¼ë¡œ ê³ ì† ì¶”ë¡ 
- Dockerë¡œ ì»¨í…Œì´ë„ˆí™”



**Happy Training! ğŸš€**
