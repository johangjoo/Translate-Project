"""
Windowsìš© Qwen LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”)
- MAX_LENGTH: 512 â†’ 160 (3ë°° ë¹ ë¦„)
- ë°°ì¹˜ í¬ê¸° ì¦ê°€
- ì˜ˆìƒ ì‹œê°„: 62ì‹œê°„ â†’ 15-20ì‹œê°„
"""
import torch
import gc
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent

# ==========================
# ì„¤ì • (ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”)
# ==========================
# ğŸ”¥ í•µì‹¬: MAX_LENGTH ëŒ€í­ ì¶•ì†Œ (512 â†’ 160)
# ì§„ë‹¨ ê²°ê³¼: 95ë°±ë¶„ìœ„ 124í† í°, í‰ê·  88í† í°
# 160ì´ë©´ 95% ì»¤ë²„ + 20% ì—¬ìœ 
MAX_SEQ_LENGTH = 160

LOAD_IN_4BIT = True
SAMPLE_RATIO = 0.1

# LoRA ì„¤ì •
LORA_R = 16
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]

# ëª¨ë¸ ì„¤ì •
MODEL_NAME = "Qwen/Qwen3-8b"
OUTPUT_DIR = str(PROJECT_ROOT / "qwen3-8b-lora-10ratio")
NEW_MODEL_NAME = "qwen3-8b-lora-10ratio"

TRAIN_FILE = str(PROJECT_ROOT / "train.jsonl")
VAL_FILE = str(PROJECT_ROOT / "validation.jsonl")

# ==========================
# 1. 4bit ì–‘ìí™” ì„¤ì •
# ==========================
print("\n" + "="*60)
print("  ğŸš€ ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”")
print(f"  MAX_LENGTH: 512 â†’ {MAX_SEQ_LENGTH} (3ë°° ë¹ ë¦„)")
print(f"  ë°ì´í„° ìƒ˜í”Œë§: {SAMPLE_RATIO*100}%")
print(f"  ëª©í‘œ ì‹œê°„: 15-20ì‹œê°„")
print("="*60 + "\n")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

# ==========================
# 2. ëª¨ë¸ & Tokenizer ë¡œë“œ
# ==========================
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
print(f"   ëª¨ë¸: {MODEL_NAME}")
print(f"   4bit ì–‘ìí™”: {LOAD_IN_4BIT}\n")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

# ==========================
# 3. LoRA ì ìš©
# ==========================
print("ğŸ”§ LoRA ì–´ëŒ‘í„° ì¶”ê°€ ì¤‘...")
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("\ní•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:")
model.print_trainable_parameters()
print()

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# ==========================
# 4. ë°ì´í„°ì…‹ ë¡œë“œ
# ==========================
print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
# ==========================
# 4. ë°ì´í„°ì…‹ ë¡œë“œ
# ==========================
print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")

def formatting_prompts_func(examples):
    """
    Qwen3 messages í˜•ì‹ì„ ChatMLë¡œ ë³€í™˜
    - enable_thinking=False (ë²ˆì—­ íƒœìŠ¤í¬)
    """
    messages_list = examples["messages"]
    texts = []
    
    for messages in messages_list:
        # Qwen3ì˜ apply_chat_template ì‚¬ìš©
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False,
            enable_thinking=False  # ë²ˆì—­ íƒœìŠ¤í¬ì—ëŠ” thinking ë¶ˆí•„ìš”
        )
        texts.append(text)
    
    return {"text": texts}

dataset = load_dataset(
    "json", 
    data_files={
        "train": TRAIN_FILE,
        "validation": VAL_FILE
    },
    keep_in_memory=False
)

print(f"   ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
print(f"   ì›ë³¸ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

# ìƒ˜í”Œë§
if SAMPLE_RATIO < 1.0:
    print(f"\nğŸ“Š {SAMPLE_RATIO*100}% ìƒ˜í”Œë§ ì¤‘...")
    train_size = int(len(dataset['train']) * SAMPLE_RATIO)
    val_size = int(len(dataset['validation']) * SAMPLE_RATIO)
    
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(train_size))
    dataset["validation"] = dataset["validation"].shuffle(seed=42).select(range(val_size))
    
    print(f"   ìƒ˜í”Œë§ í›„ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
    print(f"   ìƒ˜í”Œë§ í›„ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

print()

# í¬ë§· ì ìš©
print("ğŸ”„ ë°ì´í„° í¬ë§· ë³€í™˜ ì¤‘...")
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
    batch_size=500,
    remove_columns=dataset["train"].column_names,
    desc="Formatting prompts"
)

gc.collect()
print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n")

# ==========================
# 5. í•™ìŠµ ì„¤ì • (ìµœì í™”)
# ==========================
print("âš™ï¸  í•™ìŠµ ì„¤ì • ì¤‘ (ìµœì í™”)...")

# ğŸ”¥ VRAM ì—¬ìœ (1.48GB)ê°€ ìˆìœ¼ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥
sft_args = SFTConfig(
    output_dir=OUTPUT_DIR,
    run_name=NEW_MODEL_NAME,
    
    # ğŸ”¥ ë°°ì¹˜ í¬ê¸° ì¦ê°€ (VRAM ì—¬ìœ  í™œìš©)
    per_device_train_batch_size=6,      # 4 â†’ 6
    gradient_accumulation_steps=3,       # 4 â†’ 3
    per_device_eval_batch_size=4,        # 2 â†’ 4
    
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    
    num_train_epochs=2,
    max_steps=-1,
    
    eval_strategy="steps",
    eval_steps=1000,
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=2,
    logging_steps=100,
    logging_dir=f"{OUTPUT_DIR}/logs",
    
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    
    optim="adamw_8bit",
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    seed=42,
    report_to="tensorboard",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    
    # ğŸ”¥ ìµœì í™” í•µì‹¬
    dataset_text_field="text",
    max_length=MAX_SEQ_LENGTH,           # 160 (3ë°° ë¹ ë¦„)
    packing=True,                        # íŒ¨í‚¹ ìœ ì§€
    
    dataloader_num_workers=0,            # RAM ì ˆì•½
    dataloader_pin_memory=False,
    
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
)

# ==========================
# 6. Trainer ì´ˆê¸°í™”
# ==========================
print("ğŸ¯ Trainer ì´ˆê¸°í™” ì¤‘...\n")

trainer = SFTTrainer(
    model=model,
    args=sft_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    processing_class=tokenizer,
)

# ==========================
# 7. ì˜ˆìƒ ì‹œê°„ ì¶œë ¥
# ==========================
if torch.cuda.is_available():
    total_samples = len(dataset['train']) * sft_args.num_train_epochs
    effective_batch = sft_args.per_device_train_batch_size * sft_args.gradient_accumulation_steps
    total_steps = total_samples // effective_batch
    
    # íŒ¨í‚¹ íš¨ê³¼
    if sft_args.packing:
        total_steps = int(total_steps * 0.5)  # ì§„ë‹¨ì—ì„œ 2ë°° íš¨ìœ¨ í™•ì¸
    
    # ì§„ë‹¨ì—ì„œ ì¸¡ì •í•œ ìŠ¤í… ì‹œê°„: 25.3ì´ˆ (MAX_LENGTH=512)
    # MAX_LENGTHë¥¼ 160ìœ¼ë¡œ ì¤„ì´ë©´: 512/160 = 3.2ë°° ë¹ ë¦„
    estimated_step_time = 25.3 / 3.2  # ì•½ 7.9ì´ˆ
    estimated_hours = total_steps * estimated_step_time / 3600
    
    print("="*60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print("="*60)
    print(f"\nğŸ’¾ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    print(f"\nğŸ“Š ìµœì í™” ë‚´ìš©:")
    print(f"   MAX_LENGTH: 512 â†’ {MAX_SEQ_LENGTH} (3.2ë°° ë¹ ë¦„)")
    print(f"   ë°°ì¹˜ í¬ê¸°: 4Ã—4=16 â†’ 6Ã—3=18 (1.1ë°° ë¹ ë¦„)")
    print(f"   ì´ ê°œì„ : ì•½ 3.5ë°° ë¹ ë¦„")
    print(f"\nğŸ“Š í•™ìŠµ ì„¤ì •:")
    print(f"   ë°ì´í„°: {len(dataset['train']):,}ê°œ ({SAMPLE_RATIO*100}%)")
    print(f"   ì—í­: {sft_args.num_train_epochs}")
    print(f"   ì‹¤ì§ˆ ë°°ì¹˜: {effective_batch}")
    print(f"   ìµœëŒ€ ê¸¸ì´: {MAX_SEQ_LENGTH}")
    print(f"   íŒ¨í‚¹: âœ… í™œì„±í™”")
    print(f"\nâ±ï¸  ì˜ˆìƒ:")
    print(f"   ê¸°ì¡´ ì˜ˆìƒ: 62.4ì‹œê°„")
    print(f"   ê°œì„  ì˜ˆìƒ: ~{estimated_hours:.1f}ì‹œê°„")
    print(f"   ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}\n")
    print("="*60 + "\n")

trainer_stats = trainer.train()

# ==========================
# 8. ëª¨ë¸ ì €ì¥
# ==========================
print("\n" + "="*60)
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
print("="*60 + "\n")

lora_path = f"{OUTPUT_DIR}/lora_adapters"
model.save_pretrained(lora_path)
tokenizer.save_pretrained(lora_path)
print(f"âœ… LoRA ì–´ëŒ‘í„° ì €ì¥: {lora_path}")

print("\nğŸ”„ LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•© ì¤‘...")
merged_model = model.merge_and_unload()
merged_path = f"{OUTPUT_DIR}/{NEW_MODEL_NAME}"
merged_model.save_pretrained(merged_path)
tokenizer.save_pretrained(merged_path)
print(f"âœ… ë³‘í•© ëª¨ë¸ ì €ì¥: {merged_path}")

print("\n" + "="*60)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print("="*60)
print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
print(f"   1. LoRA ì–´ëŒ‘í„°: {lora_path}")
print(f"   2. ë³‘í•© ëª¨ë¸: {merged_path}")
print(f"   3. TensorBoard ë¡œê·¸: {OUTPUT_DIR}/logs")
print(f"\nğŸ“Š TensorBoard ì‹¤í–‰:")
print(f"   tensorboard --logdir={OUTPUT_DIR}/logs")
print("\n" + "="*60)