"""
Windowsìš© Qwen3 LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (TRL 0.23 - ì˜¬ë°”ë¥¸ ë°©ì‹)
"""
import torch
import gc
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig  # â† SFTConfig ì‚¬ìš©!

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent

# ì„¤ì •
MAX_SEQ_LENGTH = 160
LOAD_IN_4BIT = True
SAMPLE_RATIO = 0.2

LORA_R = 16
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj", 
                  "gate_proj", "up_proj", "down_proj"]

MODEL_NAME = "Qwen/Qwen3-14B"
OUTPUT_DIR = str(PROJECT_ROOT / "qwen3-14b-lora-20ratio")
NEW_MODEL_NAME = "qwen3-14b-lora-20ratio"

TRAIN_FILE = str(PROJECT_ROOT / "train.jsonl")
VAL_FILE = str(PROJECT_ROOT / "validation.jsonl")

print("\n" + "="*70)
print("  ğŸš€ Qwen3 LoRA í•™ìŠµ - RTX 5070 Ti Notebook (12GB)")
print("="*70 + "\n")

# 1. ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
)

# 2. ëª¨ë¸ & Tokenizer ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\n")

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

# 3. LoRA ì ìš©
print("ğŸ”§ LoRA ì–´ëŒ‘í„° ì¶”ê°€ ì¤‘...")
model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("\ní•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°:")
model.print_trainable_parameters()
print()

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# 4. ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")

def formatting_prompts_func(examples):
    """
    Qwen3 messages í˜•ì‹ì„ ChatMLë¡œ ë³€í™˜
    - enable_thinking=False (ë²ˆì—­ íƒœìŠ¤í¬)
    """
    messages_list = examples["messages"]
    texts = []
    
    for messages in messages_list:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False,
            enable_thinking=False
        )
        texts.append(text)
    
    return {"text": texts}

dataset = load_dataset(
    "json", 
    data_files={
        "train": TRAIN_FILE,
        "validation": VAL_FILE
    },
    keep_in_memory=False
)

print(f"   ì›ë³¸ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
print(f"   ì›ë³¸ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

# ìƒ˜í”Œë§
if SAMPLE_RATIO < 1.0:
    print(f"\nğŸ“Š {SAMPLE_RATIO*100}% ìƒ˜í”Œë§ ì¤‘...")
    train_size = int(len(dataset['train']) * SAMPLE_RATIO)
    val_size = int(len(dataset['validation']) * SAMPLE_RATIO)
    
    dataset["train"] = dataset["train"].shuffle(seed=42).select(range(train_size))
    dataset["validation"] = dataset["validation"].shuffle(seed=42).select(range(val_size))
    
    print(f"   ìƒ˜í”Œë§ í›„ í•™ìŠµ ë°ì´í„°: {len(dataset['train']):,}ê°œ")
    print(f"   ìƒ˜í”Œë§ í›„ ê²€ì¦ ë°ì´í„°: {len(dataset['validation']):,}ê°œ")

print()

# í¬ë§· ì ìš©
print("ğŸ”„ ë°ì´í„° í¬ë§· ë³€í™˜ ì¤‘...")
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
    batch_size=500,
    remove_columns=dataset["train"].column_names,
    desc="Formatting prompts"
)

gc.collect()
print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n")

# 5. í•™ìŠµ ì„¤ì • (SFTConfig ì‚¬ìš©!)
print("âš™ï¸  í•™ìŠµ ì„¤ì • ì¤‘ (12GB VRAM ìµœì í™”)...\n")

sft_args = SFTConfig(
    output_dir=OUTPUT_DIR,
    run_name=NEW_MODEL_NAME,
    
    # ë°°ì¹˜ í¬ê¸°
    per_device_train_batch_size=6,
    gradient_accumulation_steps=3,
    per_device_eval_batch_size=4,
    
    # í•™ìŠµë¥ 
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    
    # ì—í­
    num_train_epochs=2,
    max_steps=-1,
    
    # í‰ê°€ ë° ì €ì¥
    eval_strategy="steps",
    eval_steps=1000,
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=2,
    logging_steps=100,
    logging_dir=f"{OUTPUT_DIR}/logs",
    
    # ì •ë°€ë„
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    
    # ì˜µí‹°ë§ˆì´ì €
    optim="adamw_8bit",
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # ê¸°íƒ€
    seed=42,
    report_to="tensorboard",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    
    # ğŸ”¥ SFT ì „ìš© íŒŒë¼ë¯¸í„°
    dataset_text_field="text",
    max_length=MAX_SEQ_LENGTH,  # 160
    packing=True,
    
    # CPU/RAM ìµœì í™”
    dataloader_num_workers=0,
    dataloader_pin_memory=False,
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
)

# 6. Trainer ì´ˆê¸°í™”
print("ğŸ¯ Trainer ì´ˆê¸°í™” ì¤‘...\n")

trainer = SFTTrainer(
    model=model,
    args=sft_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    processing_class=tokenizer,
)

# 7. í•™ìŠµ ì‹œì‘
print("="*70)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*70)
print(f"\nğŸ’¾ GPU: {torch.cuda.get_device_name(0)}")
print(f"   ë°ì´í„°: {len(dataset['train']):,}ê°œ")
print(f"   ì—í­: 2")
print(f"   MAX_LENGTH: {MAX_SEQ_LENGTH}")
print("="*70 + "\n")

trainer_stats = trainer.train()

# 8. ëª¨ë¸ ì €ì¥
print("\n" + "="*70)
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
print("="*70 + "\n")

lora_path = f"{OUTPUT_DIR}/lora_adapters"
model.save_pretrained(lora_path)
tokenizer.save_pretrained(lora_path)
print(f"âœ… LoRA ì–´ëŒ‘í„°: {lora_path}")

merged_model = model.merge_and_unload()
merged_path = f"{OUTPUT_DIR}/{NEW_MODEL_NAME}"
merged_model.save_pretrained(merged_path)
tokenizer.save_pretrained(merged_path)
print(f"âœ… ë³‘í•© ëª¨ë¸: {merged_path}")

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"ğŸ“Š TensorBoard: tensorboard --logdir={OUTPUT_DIR}/logs\n")